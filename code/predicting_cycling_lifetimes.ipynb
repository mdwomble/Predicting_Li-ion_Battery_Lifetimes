{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Cycling Lifetimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import auc\n",
    "import math\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data file pathways\n",
    "project_path=r'C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems'\n",
    "a123_path=project_path+r'\\project_data\\A123'\n",
    "a123_timeseries=project_path+r'\\project_data\\A123\\timeseries_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>dq/dv_shift</th>\n",
       "      <th>dq_diff</th>\n",
       "      <th>max_temp_diff</th>\n",
       "      <th>ave_temp_cycle1</th>\n",
       "      <th>cycle_lifetime</th>\n",
       "      <th>2.0-2.1 V</th>\n",
       "      <th>2.1-2.2 V</th>\n",
       "      <th>2.2-2.3 V</th>\n",
       "      <th>2.3-2.4 V</th>\n",
       "      <th>...</th>\n",
       "      <th>dt_39_40 (C)</th>\n",
       "      <th>dt_40_41 (C)</th>\n",
       "      <th>dt_41_42 (C)</th>\n",
       "      <th>dt_42_43 (C)</th>\n",
       "      <th>dt_43_44 (C)</th>\n",
       "      <th>dt_44_45 (C)</th>\n",
       "      <th>dt_45_46 (C)</th>\n",
       "      <th>dt_46_47 (C)</th>\n",
       "      <th>dt_47_48 (C)</th>\n",
       "      <th>dt_48_49 (C)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a123</td>\n",
       "      <td>-0.001527</td>\n",
       "      <td>-0.008349</td>\n",
       "      <td>-6.414988</td>\n",
       "      <td>31.875011</td>\n",
       "      <td>279.0</td>\n",
       "      <td>256.411497</td>\n",
       "      <td>2.559145</td>\n",
       "      <td>3.851198</td>\n",
       "      <td>5.863147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a123</td>\n",
       "      <td>-0.001094</td>\n",
       "      <td>-0.005910</td>\n",
       "      <td>-4.915979</td>\n",
       "      <td>31.668844</td>\n",
       "      <td>2158.0</td>\n",
       "      <td>249.156107</td>\n",
       "      <td>2.684140</td>\n",
       "      <td>4.039452</td>\n",
       "      <td>6.070350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a123</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.005519</td>\n",
       "      <td>-6.163752</td>\n",
       "      <td>32.939408</td>\n",
       "      <td>786.0</td>\n",
       "      <td>260.328892</td>\n",
       "      <td>2.447698</td>\n",
       "      <td>3.434162</td>\n",
       "      <td>5.382598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a123</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>-6.523652</td>\n",
       "      <td>33.187719</td>\n",
       "      <td>288.0</td>\n",
       "      <td>248.257822</td>\n",
       "      <td>3.131025</td>\n",
       "      <td>4.263192</td>\n",
       "      <td>6.208157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a123</td>\n",
       "      <td>-0.000448</td>\n",
       "      <td>-0.003248</td>\n",
       "      <td>-6.944896</td>\n",
       "      <td>33.138162</td>\n",
       "      <td>717.0</td>\n",
       "      <td>253.051512</td>\n",
       "      <td>2.602178</td>\n",
       "      <td>3.627373</td>\n",
       "      <td>5.603975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset  dq/dv_shift   dq_diff  max_temp_diff  ave_temp_cycle1  \\\n",
       "0    a123    -0.001527 -0.008349      -6.414988        31.875011   \n",
       "1    a123    -0.001094 -0.005910      -4.915979        31.668844   \n",
       "2    a123    -0.000719 -0.005519      -6.163752        32.939408   \n",
       "3    a123     0.000180  0.001645      -6.523652        33.187719   \n",
       "4    a123    -0.000448 -0.003248      -6.944896        33.138162   \n",
       "\n",
       "   cycle_lifetime   2.0-2.1 V  2.1-2.2 V  2.2-2.3 V  2.3-2.4 V  ...  \\\n",
       "0           279.0  256.411497   2.559145   3.851198   5.863147  ...   \n",
       "1          2158.0  249.156107   2.684140   4.039452   6.070350  ...   \n",
       "2           786.0  260.328892   2.447698   3.434162   5.382598  ...   \n",
       "3           288.0  248.257822   3.131025   4.263192   6.208157  ...   \n",
       "4           717.0  253.051512   2.602178   3.627373   5.603975  ...   \n",
       "\n",
       "   dt_39_40 (C)  dt_40_41 (C)  dt_41_42 (C)  dt_42_43 (C)  dt_43_44 (C)  \\\n",
       "0           0.0           0.0           0.0           0.0           0.0   \n",
       "1           0.0           0.0           0.0           0.0           0.0   \n",
       "2           0.0           0.0           0.0           0.0           0.0   \n",
       "3           0.0           0.0           0.0           0.0           0.0   \n",
       "4           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "   dt_44_45 (C)  dt_45_46 (C)  dt_46_47 (C)  dt_47_48 (C)  dt_48_49 (C)  \n",
       "0           0.0           0.0           0.0           0.0           0.0  \n",
       "1           0.0           0.0           0.0           0.0           0.0  \n",
       "2           0.0           0.0           0.0           0.0           0.0  \n",
       "3           0.0           0.0           0.0           0.0           0.0  \n",
       "4           0.0           0.0           0.0           0.0           0.0  \n",
       "\n",
       "[5 rows x 280 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataframe from csv\n",
    "data_df=pd.read_csv(project_path+'\\\\extracted_features\\\\extracted_features_cycle100.csv')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>dq/dv_shift</th>\n",
       "      <th>dq_diff</th>\n",
       "      <th>max_temp_diff</th>\n",
       "      <th>ave_temp_cycle1</th>\n",
       "      <th>cycle_lifetime</th>\n",
       "      <th>2.0-2.1 V</th>\n",
       "      <th>2.1-2.2 V</th>\n",
       "      <th>2.2-2.3 V</th>\n",
       "      <th>2.3-2.4 V</th>\n",
       "      <th>...</th>\n",
       "      <th>dt_39_40 (C)</th>\n",
       "      <th>dt_40_41 (C)</th>\n",
       "      <th>dt_41_42 (C)</th>\n",
       "      <th>dt_42_43 (C)</th>\n",
       "      <th>dt_43_44 (C)</th>\n",
       "      <th>dt_44_45 (C)</th>\n",
       "      <th>dt_45_46 (C)</th>\n",
       "      <th>dt_46_47 (C)</th>\n",
       "      <th>dt_47_48 (C)</th>\n",
       "      <th>dt_48_49 (C)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a123</td>\n",
       "      <td>-0.001527</td>\n",
       "      <td>-0.008349</td>\n",
       "      <td>-6.414988</td>\n",
       "      <td>31.875011</td>\n",
       "      <td>279.0</td>\n",
       "      <td>256.411497</td>\n",
       "      <td>2.559145</td>\n",
       "      <td>3.851198</td>\n",
       "      <td>5.863147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a123</td>\n",
       "      <td>-0.001094</td>\n",
       "      <td>-0.005910</td>\n",
       "      <td>-4.915979</td>\n",
       "      <td>31.668844</td>\n",
       "      <td>2158.0</td>\n",
       "      <td>249.156107</td>\n",
       "      <td>2.684140</td>\n",
       "      <td>4.039452</td>\n",
       "      <td>6.070350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a123</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.005519</td>\n",
       "      <td>-6.163752</td>\n",
       "      <td>32.939408</td>\n",
       "      <td>786.0</td>\n",
       "      <td>260.328892</td>\n",
       "      <td>2.447698</td>\n",
       "      <td>3.434162</td>\n",
       "      <td>5.382598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a123</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>-6.523652</td>\n",
       "      <td>33.187719</td>\n",
       "      <td>288.0</td>\n",
       "      <td>248.257822</td>\n",
       "      <td>3.131025</td>\n",
       "      <td>4.263192</td>\n",
       "      <td>6.208157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a123</td>\n",
       "      <td>-0.000448</td>\n",
       "      <td>-0.003248</td>\n",
       "      <td>-6.944896</td>\n",
       "      <td>33.138162</td>\n",
       "      <td>717.0</td>\n",
       "      <td>253.051512</td>\n",
       "      <td>2.602178</td>\n",
       "      <td>3.627373</td>\n",
       "      <td>5.603975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset  dq/dv_shift   dq_diff  max_temp_diff  ave_temp_cycle1  \\\n",
       "0    a123    -0.001527 -0.008349      -6.414988        31.875011   \n",
       "1    a123    -0.001094 -0.005910      -4.915979        31.668844   \n",
       "2    a123    -0.000719 -0.005519      -6.163752        32.939408   \n",
       "3    a123     0.000180  0.001645      -6.523652        33.187719   \n",
       "4    a123    -0.000448 -0.003248      -6.944896        33.138162   \n",
       "\n",
       "   cycle_lifetime   2.0-2.1 V  2.1-2.2 V  2.2-2.3 V  2.3-2.4 V  ...  \\\n",
       "0           279.0  256.411497   2.559145   3.851198   5.863147  ...   \n",
       "1          2158.0  249.156107   2.684140   4.039452   6.070350  ...   \n",
       "2           786.0  260.328892   2.447698   3.434162   5.382598  ...   \n",
       "3           288.0  248.257822   3.131025   4.263192   6.208157  ...   \n",
       "4           717.0  253.051512   2.602178   3.627373   5.603975  ...   \n",
       "\n",
       "   dt_39_40 (C)  dt_40_41 (C)  dt_41_42 (C)  dt_42_43 (C)  dt_43_44 (C)  \\\n",
       "0           0.0           0.0           0.0           0.0           0.0   \n",
       "1           0.0           0.0           0.0           0.0           0.0   \n",
       "2           0.0           0.0           0.0           0.0           0.0   \n",
       "3           0.0           0.0           0.0           0.0           0.0   \n",
       "4           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "   dt_44_45 (C)  dt_45_46 (C)  dt_46_47 (C)  dt_47_48 (C)  dt_48_49 (C)  \n",
       "0           0.0           0.0           0.0           0.0           0.0  \n",
       "1           0.0           0.0           0.0           0.0           0.0  \n",
       "2           0.0           0.0           0.0           0.0           0.0  \n",
       "3           0.0           0.0           0.0           0.0           0.0  \n",
       "4           0.0           0.0           0.0           0.0           0.0  \n",
       "\n",
       "[5 rows x 280 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a123 dataframe\n",
    "data='a123'\n",
    "data_df=data_df[data_df['dataset']==data]\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop Identifying Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dq/dv_shift</th>\n",
       "      <th>dq_diff</th>\n",
       "      <th>max_temp_diff</th>\n",
       "      <th>ave_temp_cycle1</th>\n",
       "      <th>cycle_lifetime</th>\n",
       "      <th>2.0-2.1 V</th>\n",
       "      <th>2.1-2.2 V</th>\n",
       "      <th>2.2-2.3 V</th>\n",
       "      <th>2.3-2.4 V</th>\n",
       "      <th>2.4-2.5 V</th>\n",
       "      <th>...</th>\n",
       "      <th>dt_39_40 (C)</th>\n",
       "      <th>dt_40_41 (C)</th>\n",
       "      <th>dt_41_42 (C)</th>\n",
       "      <th>dt_42_43 (C)</th>\n",
       "      <th>dt_43_44 (C)</th>\n",
       "      <th>dt_44_45 (C)</th>\n",
       "      <th>dt_45_46 (C)</th>\n",
       "      <th>dt_46_47 (C)</th>\n",
       "      <th>dt_47_48 (C)</th>\n",
       "      <th>dt_48_49 (C)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001527</td>\n",
       "      <td>-0.008349</td>\n",
       "      <td>-6.414988</td>\n",
       "      <td>31.875011</td>\n",
       "      <td>279.0</td>\n",
       "      <td>256.411497</td>\n",
       "      <td>2.559145</td>\n",
       "      <td>3.851198</td>\n",
       "      <td>5.863147</td>\n",
       "      <td>8.904745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001094</td>\n",
       "      <td>-0.005910</td>\n",
       "      <td>-4.915979</td>\n",
       "      <td>31.668844</td>\n",
       "      <td>2158.0</td>\n",
       "      <td>249.156107</td>\n",
       "      <td>2.684140</td>\n",
       "      <td>4.039452</td>\n",
       "      <td>6.070350</td>\n",
       "      <td>9.006150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.005519</td>\n",
       "      <td>-6.163752</td>\n",
       "      <td>32.939408</td>\n",
       "      <td>786.0</td>\n",
       "      <td>260.328892</td>\n",
       "      <td>2.447698</td>\n",
       "      <td>3.434162</td>\n",
       "      <td>5.382598</td>\n",
       "      <td>8.503038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>-6.523652</td>\n",
       "      <td>33.187719</td>\n",
       "      <td>288.0</td>\n",
       "      <td>248.257822</td>\n",
       "      <td>3.131025</td>\n",
       "      <td>4.263192</td>\n",
       "      <td>6.208157</td>\n",
       "      <td>9.421802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000448</td>\n",
       "      <td>-0.003248</td>\n",
       "      <td>-6.944896</td>\n",
       "      <td>33.138162</td>\n",
       "      <td>717.0</td>\n",
       "      <td>253.051512</td>\n",
       "      <td>2.602178</td>\n",
       "      <td>3.627373</td>\n",
       "      <td>5.603975</td>\n",
       "      <td>8.831058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 279 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   dq/dv_shift   dq_diff  max_temp_diff  ave_temp_cycle1  cycle_lifetime  \\\n",
       "0    -0.001527 -0.008349      -6.414988        31.875011           279.0   \n",
       "1    -0.001094 -0.005910      -4.915979        31.668844          2158.0   \n",
       "2    -0.000719 -0.005519      -6.163752        32.939408           786.0   \n",
       "3     0.000180  0.001645      -6.523652        33.187719           288.0   \n",
       "4    -0.000448 -0.003248      -6.944896        33.138162           717.0   \n",
       "\n",
       "    2.0-2.1 V  2.1-2.2 V  2.2-2.3 V  2.3-2.4 V  2.4-2.5 V  ...  dt_39_40 (C)  \\\n",
       "0  256.411497   2.559145   3.851198   5.863147   8.904745  ...           0.0   \n",
       "1  249.156107   2.684140   4.039452   6.070350   9.006150  ...           0.0   \n",
       "2  260.328892   2.447698   3.434162   5.382598   8.503038  ...           0.0   \n",
       "3  248.257822   3.131025   4.263192   6.208157   9.421802  ...           0.0   \n",
       "4  253.051512   2.602178   3.627373   5.603975   8.831058  ...           0.0   \n",
       "\n",
       "   dt_40_41 (C)  dt_41_42 (C)  dt_42_43 (C)  dt_43_44 (C)  dt_44_45 (C)  \\\n",
       "0           0.0           0.0           0.0           0.0           0.0   \n",
       "1           0.0           0.0           0.0           0.0           0.0   \n",
       "2           0.0           0.0           0.0           0.0           0.0   \n",
       "3           0.0           0.0           0.0           0.0           0.0   \n",
       "4           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "   dt_45_46 (C)  dt_46_47 (C)  dt_47_48 (C)  dt_48_49 (C)  \n",
       "0           0.0           0.0           0.0           0.0  \n",
       "1           0.0           0.0           0.0           0.0  \n",
       "2           0.0           0.0           0.0           0.0  \n",
       "3           0.0           0.0           0.0           0.0  \n",
       "4           0.0           0.0           0.0           0.0  \n",
       "\n",
       "[5 rows x 279 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.drop('dataset', axis=1, inplace=True)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop Rows with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124, 279)\n"
     ]
    }
   ],
   "source": [
    "# get shape of dataframe before cleaning\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df=data_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124, 279)\n"
     ]
    }
   ],
   "source": [
    "# get shape of dataframe after cleaning\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split into features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target feature\n",
    "target='cycle_lifetime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target dataframe\n",
    "y=data_df[target].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature lists\n",
    "full_dsch_features=['dQ/dV_shift', 'diff_v_cap_area']\n",
    "voltage_features=['3.1-3.2V_time', '3.4-3.5V_time']\n",
    "temp_features=['diff_max_temp']\n",
    "\n",
    "# model feature lists\n",
    "full_model=full_dsch_features + voltage_features + temp_features\n",
    "dsch_model=full_dsch_features\n",
    "volt_temp_model=voltage_features + temp_features\n",
    "\n",
    "# create data frame with desired features\n",
    "x=data_df.drop(['cycle_time', 'cycle_lifetimes'], axis=1)\n",
    "x=x[full_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dQ/dV_shift</th>\n",
       "      <th>diff_v_cap_area</th>\n",
       "      <th>3.1-3.2V_time</th>\n",
       "      <th>3.4-3.5V_time</th>\n",
       "      <th>diff_max_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.115035</td>\n",
       "      <td>0.260998</td>\n",
       "      <td>0.204144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001116</td>\n",
       "      <td>0.014601</td>\n",
       "      <td>0.109783</td>\n",
       "      <td>0.229397</td>\n",
       "      <td>-0.251766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001634</td>\n",
       "      <td>0.032736</td>\n",
       "      <td>0.109327</td>\n",
       "      <td>0.204889</td>\n",
       "      <td>-0.636566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000931</td>\n",
       "      <td>0.019042</td>\n",
       "      <td>0.109564</td>\n",
       "      <td>0.196298</td>\n",
       "      <td>-1.013516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.001225</td>\n",
       "      <td>0.037464</td>\n",
       "      <td>0.112635</td>\n",
       "      <td>0.176408</td>\n",
       "      <td>-0.829144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dQ/dV_shift  diff_v_cap_area  3.1-3.2V_time  3.4-3.5V_time  diff_max_temp\n",
       "0     0.001100         0.000316       0.115035       0.260998       0.204144\n",
       "1    -0.001116         0.014601       0.109783       0.229397      -0.251766\n",
       "2    -0.001634         0.032736       0.109327       0.204889      -0.636566\n",
       "3    -0.000931         0.019042       0.109564       0.196298      -1.013516\n",
       "4    -0.001225         0.037464       0.112635       0.176408      -0.829144"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "x_norm=scaler.fit_transform(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data by row\n",
    "x_norm=normalize(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split into Training, Validation, and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The random seed\n",
    "random_seed=42\n",
    "\n",
    "# Set the random seed in tensor flow\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_norm, y, train_size=0.8, random_state=random_seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 279) (99,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train dataset into train and validation (skip if using cross validation)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.75, random_state=random_seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate, alpha, nodes):\n",
    "    # Build Model\n",
    "    model=tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(nodes, activation='relu', kernel_regularizer=keras.regularizers.l2(alpha))),\n",
    "    #model.add(tf.keras.layers.Dropout(0.2)),\n",
    "    model.add(tf.keras.layers.Dense(nodes, activation='relu', kernel_regularizer=keras.regularizers.l2(alpha))),\n",
    "    #model.add(tf.keras.layers.Dropout(0.2)),\n",
    "    model.add(tf.keras.layers.Dense(nodes, activation='relu', kernel_regularizer=keras.regularizers.l2(alpha))),\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error',\n",
    "              optimizer=keras.optimizers.Adam(learning_rate))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 793463.9375\n",
      "Epoch 1: val_loss improved from inf to 476148.40625, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 1s/step - loss: 766634.7500 - val_loss: 476148.4062\n",
      "Epoch 2/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 781916.9375\n",
      "Epoch 2: val_loss improved from 476148.40625 to 97060.54688, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 1s/step - loss: 615039.8125 - val_loss: 97060.5469\n",
      "Epoch 3/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 219663.1875\n",
      "Epoch 3: val_loss did not improve from 97060.54688\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 256382.2188 - val_loss: 186801.2344\n",
      "Epoch 4/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 306367.3750\n",
      "Epoch 4: val_loss improved from 97060.54688 to 52476.10156, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 880ms/step - loss: 223350.7656 - val_loss: 52476.1016\n",
      "Epoch 5/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 204138.1250\n",
      "Epoch 5: val_loss did not improve from 52476.10156\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 178229.3125 - val_loss: 92131.7500\n",
      "Epoch 6/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 188006.0469\n",
      "Epoch 6: val_loss improved from 52476.10156 to 43322.69141, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 636ms/step - loss: 214509.9219 - val_loss: 43322.6914\n",
      "Epoch 7/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 177219.5625\n",
      "Epoch 7: val_loss did not improve from 43322.69141\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 150780.9844 - val_loss: 83392.4922\n",
      "Epoch 8/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 164726.8438\n",
      "Epoch 8: val_loss did not improve from 43322.69141\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 159108.0938 - val_loss: 52209.4453\n",
      "Epoch 9/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 106415.7344\n",
      "Epoch 9: val_loss improved from 43322.69141 to 25375.77539, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 661ms/step - loss: 126706.2422 - val_loss: 25375.7754\n",
      "Epoch 10/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 163030.7188\n",
      "Epoch 10: val_loss improved from 25375.77539 to 24488.60547, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 648ms/step - loss: 120667.4531 - val_loss: 24488.6055\n",
      "Epoch 11/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 116458.4688\n",
      "Epoch 11: val_loss improved from 24488.60547 to 18217.35352, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 687ms/step - loss: 114381.2812 - val_loss: 18217.3535\n",
      "Epoch 12/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 87424.2812\n",
      "Epoch 12: val_loss did not improve from 18217.35352\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 91673.2344 - val_loss: 37925.1641\n",
      "Epoch 13/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 101855.8125\n",
      "Epoch 13: val_loss did not improve from 18217.35352\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 92158.9297 - val_loss: 24709.3887\n",
      "Epoch 14/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 80368.1875\n",
      "Epoch 14: val_loss improved from 18217.35352 to 10021.45117, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 658ms/step - loss: 75343.8750 - val_loss: 10021.4512\n",
      "Epoch 15/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 58949.2266\n",
      "Epoch 15: val_loss improved from 10021.45117 to 9494.32129, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 805ms/step - loss: 72355.1797 - val_loss: 9494.3213\n",
      "Epoch 16/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 93388.5156\n",
      "Epoch 16: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 67292.8125 - val_loss: 18252.5781\n",
      "Epoch 17/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 54128.0312\n",
      "Epoch 17: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 51704.1328 - val_loss: 26857.7070\n",
      "Epoch 18/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 45316.1719\n",
      "Epoch 18: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 45349.1914 - val_loss: 21560.2402\n",
      "Epoch 19/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 47924.4648\n",
      "Epoch 19: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 38763.4609 - val_loss: 22499.1523\n",
      "Epoch 20/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 34002.8672\n",
      "Epoch 20: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 34605.5430 - val_loss: 30077.0898\n",
      "Epoch 21/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 33287.8555\n",
      "Epoch 21: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 28773.8457 - val_loss: 46960.8867\n",
      "Epoch 22/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 19880.7734\n",
      "Epoch 22: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 29545.0410 - val_loss: 31525.9121\n",
      "Epoch 23/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 28410.3262\n",
      "Epoch 23: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 24292.8789 - val_loss: 22855.4434\n",
      "Epoch 24/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 26669.5352\n",
      "Epoch 24: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 25206.4336 - val_loss: 26887.0371\n",
      "Epoch 25/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 21172.6309\n",
      "Epoch 25: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 21311.8770 - val_loss: 28435.0117\n",
      "Epoch 26/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 18816.4863\n",
      "Epoch 26: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 19504.7148 - val_loss: 16096.0840\n",
      "Epoch 27/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 18016.7461\n",
      "Epoch 27: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 19952.9688 - val_loss: 16115.4531\n",
      "Epoch 28/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 13598.6885\n",
      "Epoch 28: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 17447.4727 - val_loss: 25856.6738\n",
      "Epoch 29/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 17484.5820\n",
      "Epoch 29: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 16772.5684 - val_loss: 17215.3418\n",
      "Epoch 30/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 15486.4375\n",
      "Epoch 30: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 13944.1650 - val_loss: 17394.8965\n",
      "Epoch 31/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10179.0703\n",
      "Epoch 31: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 12481.8691 - val_loss: 20336.9941\n",
      "Epoch 32/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 15763.4932\n",
      "Epoch 32: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 12686.4141 - val_loss: 15541.5967\n",
      "Epoch 33/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 11915.2207\n",
      "Epoch 33: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 11045.2900 - val_loss: 13383.1895\n",
      "Epoch 34/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 11946.1279\n",
      "Epoch 34: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 10792.9258 - val_loss: 13859.8848\n",
      "Epoch 35/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 11171.3691\n",
      "Epoch 35: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 10871.4756 - val_loss: 12052.9062\n",
      "Epoch 36/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 11835.4434\n",
      "Epoch 36: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 9575.4229 - val_loss: 10340.1064\n",
      "Epoch 37/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 12512.3818\n",
      "Epoch 37: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 10465.2549 - val_loss: 10516.2793\n",
      "Epoch 38/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6968.0391\n",
      "Epoch 38: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 8601.5654 - val_loss: 12614.0615\n",
      "Epoch 39/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 11463.6865\n",
      "Epoch 39: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 9165.5684 - val_loss: 10448.8418\n",
      "Epoch 40/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5223.4614\n",
      "Epoch 40: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 8257.9639 - val_loss: 10113.8828\n",
      "Epoch 41/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10493.0996\n",
      "Epoch 41: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 8057.5474 - val_loss: 9662.7344\n",
      "Epoch 42/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7747.3735\n",
      "Epoch 42: val_loss did not improve from 9494.32129\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 7264.2852 - val_loss: 10332.1045\n",
      "Epoch 43/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9579.7539\n",
      "Epoch 43: val_loss improved from 9494.32129 to 8777.14453, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 623ms/step - loss: 7847.0200 - val_loss: 8777.1445\n",
      "Epoch 44/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5077.5034\n",
      "Epoch 44: val_loss improved from 8777.14453 to 8696.35156, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 644ms/step - loss: 6816.0708 - val_loss: 8696.3516\n",
      "Epoch 45/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8091.4824\n",
      "Epoch 45: val_loss improved from 8696.35156 to 8162.19678, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 693ms/step - loss: 6596.0024 - val_loss: 8162.1968\n",
      "Epoch 46/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8666.9014\n",
      "Epoch 46: val_loss did not improve from 8162.19678\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6304.0386 - val_loss: 8394.0713\n",
      "Epoch 47/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7250.3999\n",
      "Epoch 47: val_loss did not improve from 8162.19678\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6075.2021 - val_loss: 8269.3174\n",
      "Epoch 48/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6176.8569\n",
      "Epoch 48: val_loss did not improve from 8162.19678\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6036.5806 - val_loss: 8822.9336\n",
      "Epoch 49/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7219.4800\n",
      "Epoch 49: val_loss improved from 8162.19678 to 7850.95166, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 654ms/step - loss: 5904.1460 - val_loss: 7850.9517\n",
      "Epoch 50/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4610.5796\n",
      "Epoch 50: val_loss improved from 7850.95166 to 6798.03906, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 657ms/step - loss: 5369.0171 - val_loss: 6798.0391\n",
      "Epoch 51/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6866.5176\n",
      "Epoch 51: val_loss did not improve from 6798.03906\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 5529.4102 - val_loss: 7859.3086\n",
      "Epoch 52/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4275.3022\n",
      "Epoch 52: val_loss did not improve from 6798.03906\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5461.7300 - val_loss: 7113.2842\n",
      "Epoch 53/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5224.4272\n",
      "Epoch 53: val_loss improved from 6798.03906 to 6659.21729, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 650ms/step - loss: 5042.2217 - val_loss: 6659.2173\n",
      "Epoch 54/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6275.6719\n",
      "Epoch 54: val_loss did not improve from 6659.21729\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4751.7549 - val_loss: 8721.1211\n",
      "Epoch 55/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6547.4126\n",
      "Epoch 55: val_loss improved from 6659.21729 to 5885.06689, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 646ms/step - loss: 5210.1890 - val_loss: 5885.0669\n",
      "Epoch 56/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3518.4485\n",
      "Epoch 56: val_loss improved from 5885.06689 to 5681.84961, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 781ms/step - loss: 4648.1128 - val_loss: 5681.8496\n",
      "Epoch 57/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5051.9961\n",
      "Epoch 57: val_loss did not improve from 5681.84961\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 4185.6382 - val_loss: 7574.6890\n",
      "Epoch 58/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5711.2393\n",
      "Epoch 58: val_loss improved from 5681.84961 to 5519.82275, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 657ms/step - loss: 4557.4502 - val_loss: 5519.8228\n",
      "Epoch 59/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3385.9883\n",
      "Epoch 59: val_loss improved from 5519.82275 to 5489.16797, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 712ms/step - loss: 4252.2407 - val_loss: 5489.1680\n",
      "Epoch 60/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2667.5361\n",
      "Epoch 60: val_loss did not improve from 5489.16797\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 3712.8528 - val_loss: 5929.5264\n",
      "Epoch 61/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2706.2913\n",
      "Epoch 61: val_loss did not improve from 5489.16797\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 3689.2019 - val_loss: 5497.8320\n",
      "Epoch 62/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5185.0254\n",
      "Epoch 62: val_loss improved from 5489.16797 to 5402.73926, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 658ms/step - loss: 4456.3970 - val_loss: 5402.7393\n",
      "Epoch 63/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2433.8694\n",
      "Epoch 63: val_loss improved from 5402.73926 to 4811.22754, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 648ms/step - loss: 3896.7944 - val_loss: 4811.2275\n",
      "Epoch 64/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2328.2312\n",
      "Epoch 64: val_loss did not improve from 4811.22754\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 3406.9944 - val_loss: 4917.9194\n",
      "Epoch 65/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4227.3975\n",
      "Epoch 65: val_loss did not improve from 4811.22754\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3465.8738 - val_loss: 5658.4863\n",
      "Epoch 66/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2432.2981\n",
      "Epoch 66: val_loss did not improve from 4811.22754\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 3215.4910 - val_loss: 4853.1704\n",
      "Epoch 67/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4404.3506\n",
      "Epoch 67: val_loss did not improve from 4811.22754\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3374.9253 - val_loss: 5284.8716\n",
      "Epoch 68/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3583.7761\n",
      "Epoch 68: val_loss improved from 4811.22754 to 4290.19385, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 641ms/step - loss: 3199.4634 - val_loss: 4290.1938\n",
      "Epoch 69/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3173.2463\n",
      "Epoch 69: val_loss improved from 4290.19385 to 4209.12109, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 630ms/step - loss: 2954.7695 - val_loss: 4209.1211\n",
      "Epoch 70/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1605.7029\n",
      "Epoch 70: val_loss improved from 4209.12109 to 4032.96143, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 792ms/step - loss: 3013.4956 - val_loss: 4032.9614\n",
      "Epoch 71/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3199.2019\n",
      "Epoch 71: val_loss improved from 4032.96143 to 3751.13330, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 646ms/step - loss: 2719.4199 - val_loss: 3751.1333\n",
      "Epoch 72/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2067.0925\n",
      "Epoch 72: val_loss improved from 3751.13330 to 3665.68604, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 684ms/step - loss: 2845.3909 - val_loss: 3665.6860\n",
      "Epoch 73/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2986.7253\n",
      "Epoch 73: val_loss improved from 3665.68604 to 3448.29565, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 649ms/step - loss: 2679.8459 - val_loss: 3448.2957\n",
      "Epoch 74/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3037.0691\n",
      "Epoch 74: val_loss did not improve from 3448.29565\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 2415.6675 - val_loss: 3606.3784\n",
      "Epoch 75/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3066.5322\n",
      "Epoch 75: val_loss did not improve from 3448.29565\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 2433.3000 - val_loss: 3988.7156\n",
      "Epoch 76/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3205.3223\n",
      "Epoch 76: val_loss did not improve from 3448.29565\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 2894.0635 - val_loss: 4108.2617\n",
      "Epoch 77/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2138.5513\n",
      "Epoch 77: val_loss did not improve from 3448.29565\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 3087.4558 - val_loss: 3642.0828\n",
      "Epoch 78/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3319.2185\n",
      "Epoch 78: val_loss improved from 3448.29565 to 2752.21826, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 696ms/step - loss: 2902.4932 - val_loss: 2752.2183\n",
      "Epoch 79/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2497.9829\n",
      "Epoch 79: val_loss did not improve from 2752.21826\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2046.5095 - val_loss: 3299.8801\n",
      "Epoch 80/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3182.4456\n",
      "Epoch 80: val_loss did not improve from 2752.21826\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2641.8396 - val_loss: 3414.5215\n",
      "Epoch 81/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2273.7524\n",
      "Epoch 81: val_loss improved from 2752.21826 to 2560.37549, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 646ms/step - loss: 2756.7654 - val_loss: 2560.3755\n",
      "Epoch 82/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2431.9417\n",
      "Epoch 82: val_loss improved from 2560.37549 to 2545.72021, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 639ms/step - loss: 2022.9607 - val_loss: 2545.7202\n",
      "Epoch 83/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1148.9337\n",
      "Epoch 83: val_loss improved from 2545.72021 to 2391.06348, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 654ms/step - loss: 1908.1493 - val_loss: 2391.0635\n",
      "Epoch 84/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1060.5825\n",
      "Epoch 84: val_loss improved from 2391.06348 to 2306.36450, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 792ms/step - loss: 1836.6913 - val_loss: 2306.3645\n",
      "Epoch 85/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2373.8032\n",
      "Epoch 85: val_loss improved from 2306.36450 to 1946.38708, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 681ms/step - loss: 1825.6930 - val_loss: 1946.3871\n",
      "Epoch 86/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2163.5996\n",
      "Epoch 86: val_loss did not improve from 1946.38708\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1842.4612 - val_loss: 2039.1656\n",
      "Epoch 87/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2088.0425\n",
      "Epoch 87: val_loss did not improve from 1946.38708\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1707.1395 - val_loss: 2082.4360\n",
      "Epoch 88/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 815.0859\n",
      "Epoch 88: val_loss did not improve from 1946.38708\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1717.9053 - val_loss: 2182.5146\n",
      "Epoch 89/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2055.7256\n",
      "Epoch 89: val_loss did not improve from 1946.38708\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1719.0209 - val_loss: 1971.0778\n",
      "Epoch 90/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2096.3652\n",
      "Epoch 90: val_loss did not improve from 1946.38708\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1723.3873 - val_loss: 2084.2590\n",
      "Epoch 91/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1189.3257\n",
      "Epoch 91: val_loss did not improve from 1946.38708\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1607.6692 - val_loss: 2002.6041\n",
      "Epoch 92/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 820.8004\n",
      "Epoch 92: val_loss improved from 1946.38708 to 1801.45935, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 658ms/step - loss: 1491.4069 - val_loss: 1801.4594\n",
      "Epoch 93/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1736.9708\n",
      "Epoch 93: val_loss did not improve from 1801.45935\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1492.2235 - val_loss: 1979.0621\n",
      "Epoch 94/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1779.6383\n",
      "Epoch 94: val_loss improved from 1801.45935 to 1735.12378, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 666ms/step - loss: 1398.1466 - val_loss: 1735.1238\n",
      "Epoch 95/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1795.8788\n",
      "Epoch 95: val_loss improved from 1735.12378 to 1599.82898, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 651ms/step - loss: 1502.8041 - val_loss: 1599.8290\n",
      "Epoch 96/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1823.9634\n",
      "Epoch 96: val_loss did not improve from 1599.82898\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1365.8213 - val_loss: 1788.4631\n",
      "Epoch 97/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1664.8230\n",
      "Epoch 97: val_loss improved from 1599.82898 to 1525.18701, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 652ms/step - loss: 1376.0713 - val_loss: 1525.1870\n",
      "Epoch 98/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 769.3490\n",
      "Epoch 98: val_loss did not improve from 1525.18701\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1384.3082 - val_loss: 2302.1685\n",
      "Epoch 99/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 965.2595\n",
      "Epoch 99: val_loss improved from 1525.18701 to 1517.07104, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 633ms/step - loss: 1521.9375 - val_loss: 1517.0710\n",
      "Epoch 100/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 923.6199\n",
      "Epoch 100: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1406.3673 - val_loss: 2809.5251\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2809.5251\n",
      "Epoch 1/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 792864.1875\n",
      "Epoch 1: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 1s 135ms/step - loss: 762691.0625 - val_loss: 396425.7500\n",
      "Epoch 2/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 738645.3125\n",
      "Epoch 2: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 560012.9375 - val_loss: 25871.0039\n",
      "Epoch 3/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 147464.7656\n",
      "Epoch 3: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 346376.0000 - val_loss: 202328.1094\n",
      "Epoch 4/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 311152.8750\n",
      "Epoch 4: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 217254.7031 - val_loss: 60694.8984\n",
      "Epoch 5/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 277176.0938\n",
      "Epoch 5: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 253122.7812 - val_loss: 155008.3125\n",
      "Epoch 6/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 306925.8125\n",
      "Epoch 6: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 348623.6250 - val_loss: 122089.1875\n",
      "Epoch 7/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 338653.3125\n",
      "Epoch 7: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 282660.6562 - val_loss: 21631.9336\n",
      "Epoch 8/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 160442.9219\n",
      "Epoch 8: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 149212.1094 - val_loss: 163931.3438\n",
      "Epoch 9/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 189801.8594\n",
      "Epoch 9: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 210399.4688 - val_loss: 101808.9219\n",
      "Epoch 10/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 184940.5000\n",
      "Epoch 10: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 139524.3281 - val_loss: 16251.0254\n",
      "Epoch 11/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 121172.6953\n",
      "Epoch 11: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 130256.8750 - val_loss: 31772.3320\n",
      "Epoch 12/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 145722.7500\n",
      "Epoch 12: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 158401.4375 - val_loss: 21437.7969\n",
      "Epoch 13/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 133412.4062\n",
      "Epoch 13: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 123542.2656 - val_loss: 29496.6680\n",
      "Epoch 14/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 100490.9531\n",
      "Epoch 14: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 104041.2656 - val_loss: 95261.9453\n",
      "Epoch 15/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 114632.2969\n",
      "Epoch 15: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 114357.8203 - val_loss: 48573.0547\n",
      "Epoch 16/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 109347.8984\n",
      "Epoch 16: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 80697.4609 - val_loss: 16972.6484\n",
      "Epoch 17/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 81092.3438\n",
      "Epoch 17: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 83258.2656 - val_loss: 18471.1758\n",
      "Epoch 18/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 72792.2031\n",
      "Epoch 18: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 84151.0469 - val_loss: 18799.4258\n",
      "Epoch 19/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 83934.2109\n",
      "Epoch 19: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 65528.6445 - val_loss: 49480.0156\n",
      "Epoch 20/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 61801.0859\n",
      "Epoch 20: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 60963.1055 - val_loss: 50871.5781\n",
      "Epoch 21/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 63215.7930\n",
      "Epoch 21: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 54182.5234 - val_loss: 25079.7676\n",
      "Epoch 22/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 24544.2051\n",
      "Epoch 22: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 43310.0039 - val_loss: 22613.0684\n",
      "Epoch 23/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 59472.6445\n",
      "Epoch 23: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 43424.4297 - val_loss: 25371.2891\n",
      "Epoch 24/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 41105.4258\n",
      "Epoch 24: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 34754.3438 - val_loss: 32198.5000\n",
      "Epoch 25/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 34387.0703\n",
      "Epoch 25: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 31783.8184 - val_loss: 30766.3008\n",
      "Epoch 26/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 27429.1895\n",
      "Epoch 26: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 27784.7852 - val_loss: 27268.6133\n",
      "Epoch 27/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 23743.8418\n",
      "Epoch 27: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 26993.7090 - val_loss: 27438.2500\n",
      "Epoch 28/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 23307.6973\n",
      "Epoch 28: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 25433.9570 - val_loss: 28309.4492\n",
      "Epoch 29/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 19353.9941\n",
      "Epoch 29: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 22415.4082 - val_loss: 27045.6680\n",
      "Epoch 30/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 24245.4492\n",
      "Epoch 30: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 20628.4297 - val_loss: 24225.4609\n",
      "Epoch 31/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 17075.2949\n",
      "Epoch 31: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 19648.8652 - val_loss: 22497.0098\n",
      "Epoch 32/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 22956.3340\n",
      "Epoch 32: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 17660.6660 - val_loss: 21808.9980\n",
      "Epoch 33/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 17583.1016\n",
      "Epoch 33: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 17401.2422 - val_loss: 19945.5293\n",
      "Epoch 34/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 16755.6934\n",
      "Epoch 34: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 15482.2314 - val_loss: 19166.1758\n",
      "Epoch 35/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 17033.9902\n",
      "Epoch 35: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 15332.1377 - val_loss: 17398.5020\n",
      "Epoch 36/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 16253.0205\n",
      "Epoch 36: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 14171.1465 - val_loss: 17463.0859\n",
      "Epoch 37/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 15827.3828\n",
      "Epoch 37: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 13319.4844 - val_loss: 15919.9473\n",
      "Epoch 38/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9674.6338\n",
      "Epoch 38: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 12832.7041 - val_loss: 15473.9668\n",
      "Epoch 39/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 14955.0967\n",
      "Epoch 39: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 11893.0781 - val_loss: 15607.1064\n",
      "Epoch 40/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7129.4863\n",
      "Epoch 40: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 12781.2041 - val_loss: 14741.1582\n",
      "Epoch 41/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 14639.0801\n",
      "Epoch 41: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 11131.7285 - val_loss: 14401.4199\n",
      "Epoch 42/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 11149.4219\n",
      "Epoch 42: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 11751.8096 - val_loss: 12445.8164\n",
      "Epoch 43/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 12206.0703\n",
      "Epoch 43: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 9491.6562 - val_loss: 14553.6152\n",
      "Epoch 44/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9638.0811\n",
      "Epoch 44: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 11693.4824 - val_loss: 11262.0439\n",
      "Epoch 45/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10548.4502\n",
      "Epoch 45: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 9432.5234 - val_loss: 11882.4404\n",
      "Epoch 46/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 14435.3408\n",
      "Epoch 46: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 10599.1592 - val_loss: 9808.1934\n",
      "Epoch 47/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10423.8945\n",
      "Epoch 47: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 8805.9746 - val_loss: 10835.5908\n",
      "Epoch 48/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9930.1143\n",
      "Epoch 48: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 9519.3613 - val_loss: 8826.6758\n",
      "Epoch 49/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9615.9170\n",
      "Epoch 49: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 8054.5732 - val_loss: 8379.8223\n",
      "Epoch 50/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7512.6094\n",
      "Epoch 50: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 7748.0762 - val_loss: 8133.2954\n",
      "Epoch 51/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9230.4297\n",
      "Epoch 51: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 7395.6318 - val_loss: 7696.5679\n",
      "Epoch 52/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6744.2017\n",
      "Epoch 52: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 7126.6147 - val_loss: 7291.5322\n",
      "Epoch 53/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7377.6978\n",
      "Epoch 53: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 6982.6587 - val_loss: 6947.4297\n",
      "Epoch 54/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8795.5498\n",
      "Epoch 54: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 6736.4023 - val_loss: 6749.7061\n",
      "Epoch 55/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8341.2168\n",
      "Epoch 55: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6589.5664 - val_loss: 6567.8506\n",
      "Epoch 56/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5189.7080\n",
      "Epoch 56: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 6387.4521 - val_loss: 5826.5024\n",
      "Epoch 57/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6781.9546\n",
      "Epoch 57: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 6591.1504 - val_loss: 5520.7671\n",
      "Epoch 58/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7683.1133\n",
      "Epoch 58: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6172.9141 - val_loss: 6136.4473\n",
      "Epoch 59/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5458.0269\n",
      "Epoch 59: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 6213.0757 - val_loss: 5095.3174\n",
      "Epoch 60/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3742.9214\n",
      "Epoch 60: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 6088.9004 - val_loss: 4897.4028\n",
      "Epoch 61/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5045.4507\n",
      "Epoch 61: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6009.7896 - val_loss: 4866.8311\n",
      "Epoch 62/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6447.3564\n",
      "Epoch 62: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 5486.3711 - val_loss: 4266.0659\n",
      "Epoch 63/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3434.8152\n",
      "Epoch 63: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 5270.2969 - val_loss: 4088.9446\n",
      "Epoch 64/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4217.8809\n",
      "Epoch 64: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5055.0557 - val_loss: 4148.8306\n",
      "Epoch 65/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5749.1196\n",
      "Epoch 65: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 4939.8110 - val_loss: 3791.2427\n",
      "Epoch 66/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3550.5256\n",
      "Epoch 66: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 4764.8096 - val_loss: 3683.6450\n",
      "Epoch 67/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6267.4790\n",
      "Epoch 67: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4689.7275 - val_loss: 3652.2507\n",
      "Epoch 68/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5670.0273\n",
      "Epoch 68: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 4578.0381 - val_loss: 3556.7056\n",
      "Epoch 69/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4799.7925\n",
      "Epoch 69: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 4365.1484 - val_loss: 3238.3713\n",
      "Epoch 70/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2943.7053\n",
      "Epoch 70: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 4380.4199 - val_loss: 3121.5535\n",
      "Epoch 71/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4957.0459\n",
      "Epoch 71: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 4172.7881 - val_loss: 3227.2798\n",
      "Epoch 72/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3118.7576\n",
      "Epoch 72: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 4069.9292 - val_loss: 2999.5720\n",
      "Epoch 73/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4954.0303\n",
      "Epoch 73: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 3979.8887 - val_loss: 2851.7036\n",
      "Epoch 74/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4682.5103\n",
      "Epoch 74: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 3817.6792 - val_loss: 3047.3186\n",
      "Epoch 75/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4975.7456\n",
      "Epoch 75: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 3900.6670 - val_loss: 2613.0369\n",
      "Epoch 76/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3674.4766\n",
      "Epoch 76: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 3787.1975 - val_loss: 2479.3721\n",
      "Epoch 77/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2418.7234\n",
      "Epoch 77: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 3543.1135 - val_loss: 2947.0967\n",
      "Epoch 78/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4478.7104\n",
      "Epoch 78: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3698.5032 - val_loss: 2304.3867\n",
      "Epoch 79/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3790.3618\n",
      "Epoch 79: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 3923.6218 - val_loss: 2249.3640\n",
      "Epoch 80/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3516.3459\n",
      "Epoch 80: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 3683.4956 - val_loss: 2776.4041\n",
      "Epoch 81/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2050.4429\n",
      "Epoch 81: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3294.9514 - val_loss: 2469.0913\n",
      "Epoch 82/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4293.6719\n",
      "Epoch 82: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 3927.5537 - val_loss: 2121.1282\n",
      "Epoch 83/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1525.4548\n",
      "Epoch 83: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 3220.7803 - val_loss: 2544.7102\n",
      "Epoch 84/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2365.3525\n",
      "Epoch 84: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3155.1211 - val_loss: 2487.7217\n",
      "Epoch 85/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4307.3286\n",
      "Epoch 85: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 3469.4075 - val_loss: 1986.2992\n",
      "Epoch 86/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3509.4275\n",
      "Epoch 86: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 3048.4033 - val_loss: 2036.6791\n",
      "Epoch 87/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3296.4497\n",
      "Epoch 87: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 2721.5002 - val_loss: 1910.5032\n",
      "Epoch 88/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2103.4883\n",
      "Epoch 88: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 3065.9937 - val_loss: 1651.7051\n",
      "Epoch 89/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3122.9065\n",
      "Epoch 89: val_loss did not improve from 1517.07104\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 2595.4868 - val_loss: 1897.6324\n",
      "Epoch 90/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3331.3142\n",
      "Epoch 90: val_loss improved from 1517.07104 to 1471.54126, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 786ms/step - loss: 2633.2078 - val_loss: 1471.5413\n",
      "Epoch 91/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1855.1173\n",
      "Epoch 91: val_loss improved from 1471.54126 to 1441.22095, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 653ms/step - loss: 2538.5625 - val_loss: 1441.2209\n",
      "Epoch 92/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1369.2600\n",
      "Epoch 92: val_loss improved from 1441.22095 to 1424.11072, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 681ms/step - loss: 2408.2466 - val_loss: 1424.1107\n",
      "Epoch 93/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2713.2056\n",
      "Epoch 93: val_loss improved from 1424.11072 to 1320.02551, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 692ms/step - loss: 2360.0869 - val_loss: 1320.0255\n",
      "Epoch 94/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2678.5198\n",
      "Epoch 94: val_loss did not improve from 1320.02551\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 2261.8560 - val_loss: 1445.8635\n",
      "Epoch 95/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2764.6665\n",
      "Epoch 95: val_loss improved from 1320.02551 to 1309.59241, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 654ms/step - loss: 2337.5618 - val_loss: 1309.5924\n",
      "Epoch 96/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2960.9614\n",
      "Epoch 96: val_loss improved from 1309.59241 to 1103.69653, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 661ms/step - loss: 2209.6631 - val_loss: 1103.6965\n",
      "Epoch 97/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2578.7073\n",
      "Epoch 97: val_loss did not improve from 1103.69653\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2154.8191 - val_loss: 1104.4803\n",
      "Epoch 98/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1133.1752\n",
      "Epoch 98: val_loss did not improve from 1103.69653\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2134.1379 - val_loss: 1137.1647\n",
      "Epoch 99/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 904.7916\n",
      "Epoch 99: val_loss did not improve from 1103.69653\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2006.0249 - val_loss: 1113.6105\n",
      "Epoch 100/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1523.3086\n",
      "Epoch 100: val_loss improved from 1103.69653 to 1026.47595, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 651ms/step - loss: 2130.8547 - val_loss: 1026.4760\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1026.4760\n",
      "Epoch 1/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 792993.0000\n",
      "Epoch 1: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 763239.1875 - val_loss: 589470.3125\n",
      "Epoch 2/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 744381.4375\n",
      "Epoch 2: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 567938.5625 - val_loss: 102513.3125\n",
      "Epoch 3/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 151772.5781\n",
      "Epoch 3: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 335123.0625 - val_loss: 164583.1562\n",
      "Epoch 4/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 309522.1562\n",
      "Epoch 4: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 216316.2031 - val_loss: 167953.7812\n",
      "Epoch 5/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 266559.0938\n",
      "Epoch 5: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 242087.6406 - val_loss: 283832.6250\n",
      "Epoch 6/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 290412.4688\n",
      "Epoch 6: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 329939.4375 - val_loss: 235732.5312\n",
      "Epoch 7/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 315974.3125\n",
      "Epoch 7: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 262304.0312 - val_loss: 89557.1328\n",
      "Epoch 8/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 152388.7656\n",
      "Epoch 8: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 144604.6875 - val_loss: 117884.6719\n",
      "Epoch 9/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 185411.5000\n",
      "Epoch 9: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 204415.1094 - val_loss: 77777.7344\n",
      "Epoch 10/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 180217.5781\n",
      "Epoch 10: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 135808.5312 - val_loss: 68513.0156\n",
      "Epoch 11/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 121548.4844\n",
      "Epoch 11: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 130071.6953 - val_loss: 105456.7031\n",
      "Epoch 12/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 142776.6406\n",
      "Epoch 12: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 154064.5312 - val_loss: 74961.0156\n",
      "Epoch 13/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 127187.3047\n",
      "Epoch 13: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 117265.0703 - val_loss: 39001.1250\n",
      "Epoch 14/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 99390.8984\n",
      "Epoch 14: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 104619.9219 - val_loss: 58556.6172\n",
      "Epoch 15/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 111979.8047\n",
      "Epoch 15: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 110905.5625 - val_loss: 31830.1191\n",
      "Epoch 16/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 106721.5469\n",
      "Epoch 16: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 77810.4219 - val_loss: 35314.4180\n",
      "Epoch 17/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 82491.4297\n",
      "Epoch 17: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 83942.0000 - val_loss: 38689.9180\n",
      "Epoch 18/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 69278.7656\n",
      "Epoch 18: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 78370.1172 - val_loss: 20145.7559\n",
      "Epoch 19/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 78093.7188\n",
      "Epoch 19: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 63328.5508 - val_loss: 28898.0977\n",
      "Epoch 20/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 64145.3672\n",
      "Epoch 20: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 61459.1406 - val_loss: 21100.7539\n",
      "Epoch 21/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 57668.4375\n",
      "Epoch 21: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 50186.1250 - val_loss: 12360.9834\n",
      "Epoch 22/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 24749.1152\n",
      "Epoch 22: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 43181.6445 - val_loss: 11906.4756\n",
      "Epoch 23/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 56685.4883\n",
      "Epoch 23: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 41628.3477 - val_loss: 12278.9863\n",
      "Epoch 24/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 39186.7539\n",
      "Epoch 24: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 33876.9805 - val_loss: 17707.2891\n",
      "Epoch 25/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 33932.6250\n",
      "Epoch 25: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 30851.0957 - val_loss: 13411.5527\n",
      "Epoch 26/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 27316.7441\n",
      "Epoch 26: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 27078.7051 - val_loss: 10272.9951\n",
      "Epoch 27/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 23960.0039\n",
      "Epoch 27: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 26123.9082 - val_loss: 11355.3994\n",
      "Epoch 28/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 20019.5605\n",
      "Epoch 28: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 24123.6152 - val_loss: 18129.4316\n",
      "Epoch 29/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 20395.4355\n",
      "Epoch 29: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 22413.4473 - val_loss: 10567.8672\n",
      "Epoch 30/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 23294.8730\n",
      "Epoch 30: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 19565.0254 - val_loss: 7429.4038\n",
      "Epoch 31/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 16590.9043\n",
      "Epoch 31: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 19134.5684 - val_loss: 8141.6797\n",
      "Epoch 32/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 21845.7031\n",
      "Epoch 32: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 17041.1660 - val_loss: 12356.4072\n",
      "Epoch 33/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 17900.1523\n",
      "Epoch 33: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 17169.2051 - val_loss: 7798.2085\n",
      "Epoch 34/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 16259.1748\n",
      "Epoch 34: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 15264.7881 - val_loss: 6713.8384\n",
      "Epoch 35/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 17392.2188\n",
      "Epoch 35: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 15275.6992 - val_loss: 7480.8213\n",
      "Epoch 36/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 15467.0537\n",
      "Epoch 36: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 13848.7705 - val_loss: 9405.8799\n",
      "Epoch 37/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 15220.4561\n",
      "Epoch 37: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 12866.3389 - val_loss: 6332.4580\n",
      "Epoch 38/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9255.7139\n",
      "Epoch 38: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 12700.7295 - val_loss: 5974.1060\n",
      "Epoch 39/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 14236.0469\n",
      "Epoch 39: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 11408.0264 - val_loss: 8672.4932\n",
      "Epoch 40/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6998.8857\n",
      "Epoch 40: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 12838.0811 - val_loss: 7180.3096\n",
      "Epoch 41/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 14178.9922\n",
      "Epoch 41: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 10815.7578 - val_loss: 5023.4287\n",
      "Epoch 42/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 11249.0908\n",
      "Epoch 42: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 11912.0986 - val_loss: 4814.0088\n",
      "Epoch 43/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 11602.8809\n",
      "Epoch 43: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 9105.8105 - val_loss: 11619.9092\n",
      "Epoch 44/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10228.9795\n",
      "Epoch 44: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 11875.1504 - val_loss: 4512.6064\n",
      "Epoch 45/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10107.9180\n",
      "Epoch 45: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 9227.6777 - val_loss: 4007.2051\n",
      "Epoch 46/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 14338.5918\n",
      "Epoch 46: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 10426.1836 - val_loss: 4521.2505\n",
      "Epoch 47/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10012.8262\n",
      "Epoch 47: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 8608.2881 - val_loss: 6960.9429\n",
      "Epoch 48/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9527.6865\n",
      "Epoch 48: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 9264.8223 - val_loss: 3427.4106\n",
      "Epoch 49/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9211.2207\n",
      "Epoch 49: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 7815.6904 - val_loss: 3500.3594\n",
      "Epoch 50/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6905.5293\n",
      "Epoch 50: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 7444.2007 - val_loss: 4315.5669\n",
      "Epoch 51/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8766.4756\n",
      "Epoch 51: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 7121.3145 - val_loss: 3339.2102\n",
      "Epoch 52/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6308.6025\n",
      "Epoch 52: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 6798.4131 - val_loss: 3337.0852\n",
      "Epoch 53/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7137.7646\n",
      "Epoch 53: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 6627.7178 - val_loss: 3670.8091\n",
      "Epoch 54/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8255.6152\n",
      "Epoch 54: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 6359.7925 - val_loss: 3747.8320\n",
      "Epoch 55/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7716.3179\n",
      "Epoch 55: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 6185.6514 - val_loss: 3705.0828\n",
      "Epoch 56/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4650.8857\n",
      "Epoch 56: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 6032.8306 - val_loss: 3018.9880\n",
      "Epoch 57/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6330.2690\n",
      "Epoch 57: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 6147.0366 - val_loss: 2953.8889\n",
      "Epoch 58/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6977.0396\n",
      "Epoch 58: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 5817.8013 - val_loss: 4179.5786\n",
      "Epoch 59/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4853.1104\n",
      "Epoch 59: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5688.6812 - val_loss: 2280.8584\n",
      "Epoch 60/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3574.9148\n",
      "Epoch 60: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 5745.8384 - val_loss: 2282.8594\n",
      "Epoch 61/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4320.9312\n",
      "Epoch 61: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5821.4595 - val_loss: 3239.4719\n",
      "Epoch 62/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5921.7676\n",
      "Epoch 62: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5163.4551 - val_loss: 1956.7695\n",
      "Epoch 63/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3409.7803\n",
      "Epoch 63: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 5115.3076 - val_loss: 2605.0161\n",
      "Epoch 64/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3774.2410\n",
      "Epoch 64: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 4817.8574 - val_loss: 3034.2244\n",
      "Epoch 65/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5459.9175\n",
      "Epoch 65: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4602.9722 - val_loss: 1912.1482\n",
      "Epoch 66/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3497.9526\n",
      "Epoch 66: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 4537.3174 - val_loss: 2236.2920\n",
      "Epoch 67/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5673.2881\n",
      "Epoch 67: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 4270.9316 - val_loss: 2528.9246\n",
      "Epoch 68/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5005.6089\n",
      "Epoch 68: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4168.6128 - val_loss: 1896.9597\n",
      "Epoch 69/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4415.7734\n",
      "Epoch 69: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 3955.7134 - val_loss: 1724.8958\n",
      "Epoch 70/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2635.2527\n",
      "Epoch 70: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 3968.3213 - val_loss: 1611.5673\n",
      "Epoch 71/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4457.8247\n",
      "Epoch 71: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 3743.9507 - val_loss: 1356.5820\n",
      "Epoch 72/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2912.6389\n",
      "Epoch 72: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 3673.4021 - val_loss: 1537.0326\n",
      "Epoch 73/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4369.9199\n",
      "Epoch 73: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 3533.3274 - val_loss: 1367.0039\n",
      "Epoch 74/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4204.5591\n",
      "Epoch 74: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 3418.0625 - val_loss: 1419.3021\n",
      "Epoch 75/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4193.9214\n",
      "Epoch 75: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 3428.8633 - val_loss: 1170.7195\n",
      "Epoch 76/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3368.6201\n",
      "Epoch 76: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 3323.9641 - val_loss: 1094.0427\n",
      "Epoch 77/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2157.7070\n",
      "Epoch 77: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 3135.2480 - val_loss: 1731.5215\n",
      "Epoch 78/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3947.3215\n",
      "Epoch 78: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 3115.5876 - val_loss: 1085.7501\n",
      "Epoch 79/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3286.8306\n",
      "Epoch 79: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 3329.7285 - val_loss: 1905.6250\n",
      "Epoch 80/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3343.6392\n",
      "Epoch 80: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3528.8816 - val_loss: 1246.8389\n",
      "Epoch 81/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1717.1819\n",
      "Epoch 81: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 2829.8467 - val_loss: 1394.8521\n",
      "Epoch 82/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4185.3174\n",
      "Epoch 82: val_loss did not improve from 1026.47595\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 3399.0142 - val_loss: 2285.6675\n",
      "Epoch 83/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2165.6294\n",
      "Epoch 83: val_loss improved from 1026.47595 to 857.76331, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 831ms/step - loss: 3289.6577 - val_loss: 857.7633\n",
      "Epoch 84/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1612.3909\n",
      "Epoch 84: val_loss did not improve from 857.76331\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 2716.3225 - val_loss: 928.3328\n",
      "Epoch 85/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3585.7644\n",
      "Epoch 85: val_loss did not improve from 857.76331\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 2772.8347 - val_loss: 2196.1709\n",
      "Epoch 86/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3493.4602\n",
      "Epoch 86: val_loss improved from 857.76331 to 755.19360, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 651ms/step - loss: 2922.5085 - val_loss: 755.1936\n",
      "Epoch 87/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3153.9707\n",
      "Epoch 87: val_loss did not improve from 755.19360\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 2586.6650 - val_loss: 855.0763\n",
      "Epoch 88/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1065.5717\n",
      "Epoch 88: val_loss did not improve from 755.19360\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 2628.6965 - val_loss: 824.7176\n",
      "Epoch 89/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2606.4602\n",
      "Epoch 89: val_loss did not improve from 755.19360\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 2574.5806 - val_loss: 797.6239\n",
      "Epoch 90/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2804.0637\n",
      "Epoch 90: val_loss did not improve from 755.19360\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2328.5195 - val_loss: 1544.3735\n",
      "Epoch 91/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1621.3810\n",
      "Epoch 91: val_loss did not improve from 755.19360\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 2249.2166 - val_loss: 1344.8347\n",
      "Epoch 92/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1906.3069\n",
      "Epoch 92: val_loss did not improve from 755.19360\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 2645.1721 - val_loss: 1199.6122\n",
      "Epoch 93/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2560.0195\n",
      "Epoch 93: val_loss did not improve from 755.19360\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 2266.2039 - val_loss: 856.4022\n",
      "Epoch 94/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2425.7732\n",
      "Epoch 94: val_loss improved from 755.19360 to 674.78363, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 681ms/step - loss: 2276.0391 - val_loss: 674.7836\n",
      "Epoch 95/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2222.1433\n",
      "Epoch 95: val_loss did not improve from 674.78363\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1904.0153 - val_loss: 1395.1626\n",
      "Epoch 96/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3077.0642\n",
      "Epoch 96: val_loss did not improve from 674.78363\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 2228.0337 - val_loss: 911.4714\n",
      "Epoch 97/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2640.6038\n",
      "Epoch 97: val_loss did not improve from 674.78363\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 2249.4360 - val_loss: 851.2414\n",
      "Epoch 98/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1083.5165\n",
      "Epoch 98: val_loss improved from 674.78363 to 665.98743, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 665ms/step - loss: 2200.9829 - val_loss: 665.9874\n",
      "Epoch 99/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 857.0806\n",
      "Epoch 99: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1909.2321 - val_loss: 806.5081\n",
      "Epoch 100/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1445.6068\n",
      "Epoch 100: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2159.1885 - val_loss: 826.1831\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 826.1831\n",
      "Epoch 1/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 793229.6875\n",
      "Epoch 1: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 1s 117ms/step - loss: 763616.8125 - val_loss: 309916.3438\n",
      "Epoch 2/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 740806.1250\n",
      "Epoch 2: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 561291.3750 - val_loss: 56271.3438\n",
      "Epoch 3/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 144402.9688\n",
      "Epoch 3: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 352834.6562 - val_loss: 286507.0625\n",
      "Epoch 4/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 303422.4375\n",
      "Epoch 4: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 211646.9219 - val_loss: 51333.8164\n",
      "Epoch 5/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 290694.7500\n",
      "Epoch 5: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 266123.7188 - val_loss: 122510.8516\n",
      "Epoch 6/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 322324.6875\n",
      "Epoch 6: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 365092.7812 - val_loss: 100887.8594\n",
      "Epoch 7/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 360943.7500\n",
      "Epoch 7: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 305093.5938 - val_loss: 29748.0117\n",
      "Epoch 8/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 180326.9062\n",
      "Epoch 8: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 160756.3906 - val_loss: 179183.7969\n",
      "Epoch 9/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 162728.6875\n",
      "Epoch 9: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 197433.3750 - val_loss: 197342.0938\n",
      "Epoch 10/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 213067.6250\n",
      "Epoch 10: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 165847.7031 - val_loss: 38932.1211\n",
      "Epoch 11/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 112794.8984\n",
      "Epoch 11: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 122142.8516 - val_loss: 26527.1309\n",
      "Epoch 12/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 144298.4531\n",
      "Epoch 12: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 161333.6719 - val_loss: 26203.8887\n",
      "Epoch 13/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 151894.0781\n",
      "Epoch 13: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 143589.9062 - val_loss: 27430.9961\n",
      "Epoch 14/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 113687.6094\n",
      "Epoch 14: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 105506.8281 - val_loss: 94816.9141\n",
      "Epoch 15/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 104482.2500\n",
      "Epoch 15: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 110233.2344 - val_loss: 97983.0703\n",
      "Epoch 16/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 124646.2109\n",
      "Epoch 16: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 98308.6875 - val_loss: 37099.6875\n",
      "Epoch 17/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 79765.6172\n",
      "Epoch 17: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 80096.2188 - val_loss: 20007.3438\n",
      "Epoch 18/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 73664.4922\n",
      "Epoch 18: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 89975.0469 - val_loss: 20318.7559\n",
      "Epoch 19/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 101746.5703\n",
      "Epoch 19: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 77304.8047 - val_loss: 34958.8164\n",
      "Epoch 20/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 56188.3047\n",
      "Epoch 20: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 58143.0000 - val_loss: 63754.3359\n",
      "Epoch 21/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 70923.6953\n",
      "Epoch 21: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 63155.6953 - val_loss: 47052.1250\n",
      "Epoch 22/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 31898.9121\n",
      "Epoch 22: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 49056.6562 - val_loss: 26504.4805\n",
      "Epoch 23/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 61243.9805\n",
      "Epoch 23: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 44547.9609 - val_loss: 26661.6660\n",
      "Epoch 24/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 53874.3398\n",
      "Epoch 24: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 43780.5664 - val_loss: 29512.6836\n",
      "Epoch 25/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 38430.2930\n",
      "Epoch 25: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 34234.5742 - val_loss: 39967.8867\n",
      "Epoch 26/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 30801.5000\n",
      "Epoch 26: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 33582.2891 - val_loss: 37252.3008\n",
      "Epoch 27/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 26390.9062\n",
      "Epoch 27: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 28612.6465 - val_loss: 31284.1504\n",
      "Epoch 28/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 25917.3281\n",
      "Epoch 28: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 26945.5762 - val_loss: 31286.7461\n",
      "Epoch 29/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 20994.1543\n",
      "Epoch 29: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 23740.7129 - val_loss: 32909.5156\n",
      "Epoch 30/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 25494.1680\n",
      "Epoch 30: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 22150.2266 - val_loss: 34725.8477\n",
      "Epoch 31/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 20477.4688\n",
      "Epoch 31: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 21013.7949 - val_loss: 29733.9883\n",
      "Epoch 32/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 23375.3008\n",
      "Epoch 32: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 18459.2656 - val_loss: 28107.9941\n",
      "Epoch 33/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 19387.2051\n",
      "Epoch 33: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 17757.8574 - val_loss: 28440.1465\n",
      "Epoch 34/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 17478.8184\n",
      "Epoch 34: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 16140.9150 - val_loss: 27671.5352\n",
      "Epoch 35/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 16821.3066\n",
      "Epoch 35: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 15222.1123 - val_loss: 26293.6836\n",
      "Epoch 36/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 16790.9121\n",
      "Epoch 36: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 14401.3369 - val_loss: 24649.0352\n",
      "Epoch 37/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 16832.0859\n",
      "Epoch 37: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 13767.2344 - val_loss: 23510.1738\n",
      "Epoch 38/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10354.2422\n",
      "Epoch 38: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 13178.6250 - val_loss: 23934.3809\n",
      "Epoch 39/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 15898.4434\n",
      "Epoch 39: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 12308.1709 - val_loss: 25007.5527\n",
      "Epoch 40/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7606.1890\n",
      "Epoch 40: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 12601.0312 - val_loss: 22250.0195\n",
      "Epoch 41/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 15091.2998\n",
      "Epoch 41: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 11499.1797 - val_loss: 19683.0723\n",
      "Epoch 42/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 11331.4316\n",
      "Epoch 42: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 11675.0732 - val_loss: 20779.9707\n",
      "Epoch 43/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 12917.5361\n",
      "Epoch 43: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 10000.8916 - val_loss: 24911.0117\n",
      "Epoch 44/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9358.5859\n",
      "Epoch 44: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 11514.7168 - val_loss: 20496.3828\n",
      "Epoch 45/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 11084.8691\n",
      "Epoch 45: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 9734.6553 - val_loss: 17876.0684\n",
      "Epoch 46/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 14246.0195\n",
      "Epoch 46: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 10428.7539 - val_loss: 18924.8789\n",
      "Epoch 47/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10918.9668\n",
      "Epoch 47: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 9029.5020 - val_loss: 22030.3867\n",
      "Epoch 48/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9524.6631\n",
      "Epoch 48: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 9526.6084 - val_loss: 19448.3730\n",
      "Epoch 49/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10139.4834\n",
      "Epoch 49: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 8372.4893 - val_loss: 18436.2949\n",
      "Epoch 50/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7733.1484\n",
      "Epoch 50: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 8038.3228 - val_loss: 18507.3125\n",
      "Epoch 51/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9496.8613\n",
      "Epoch 51: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 7699.3306 - val_loss: 18169.0098\n",
      "Epoch 52/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7122.5806\n",
      "Epoch 52: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 7473.1450 - val_loss: 17900.0117\n",
      "Epoch 53/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7687.0439\n",
      "Epoch 53: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 7282.6548 - val_loss: 17317.8066\n",
      "Epoch 54/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9140.3662\n",
      "Epoch 54: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 7091.0757 - val_loss: 17638.4551\n",
      "Epoch 55/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8647.0889\n",
      "Epoch 55: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 6841.6602 - val_loss: 18418.6133\n",
      "Epoch 56/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5501.9600\n",
      "Epoch 56: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 6767.7930 - val_loss: 17014.5938\n",
      "Epoch 57/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6934.1157\n",
      "Epoch 57: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 6892.1729 - val_loss: 16537.5352\n",
      "Epoch 58/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8153.1616\n",
      "Epoch 58: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 6533.4102 - val_loss: 18485.8945\n",
      "Epoch 59/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5668.0571\n",
      "Epoch 59: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6417.2349 - val_loss: 16922.8711\n",
      "Epoch 60/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3981.1152\n",
      "Epoch 60: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 6267.2183 - val_loss: 16025.1230\n",
      "Epoch 61/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5301.8315\n",
      "Epoch 61: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6197.4058 - val_loss: 16817.3770\n",
      "Epoch 62/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6569.8496\n",
      "Epoch 62: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 5696.7515 - val_loss: 15895.7090\n",
      "Epoch 63/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3613.2864\n",
      "Epoch 63: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5512.5303 - val_loss: 15959.4316\n",
      "Epoch 64/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4526.6509\n",
      "Epoch 64: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 5329.0361 - val_loss: 16004.2998\n",
      "Epoch 65/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5957.7314\n",
      "Epoch 65: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 5181.7744 - val_loss: 14906.9688\n",
      "Epoch 66/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3702.0222\n",
      "Epoch 66: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5025.4204 - val_loss: 14628.7842\n",
      "Epoch 67/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6574.6572\n",
      "Epoch 67: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 4938.5103 - val_loss: 14754.3496\n",
      "Epoch 68/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5975.5093\n",
      "Epoch 68: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 4811.3252 - val_loss: 14750.9951\n",
      "Epoch 69/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5060.4897\n",
      "Epoch 69: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4640.9604 - val_loss: 13846.4121\n",
      "Epoch 70/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3088.7637\n",
      "Epoch 70: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 4597.6719 - val_loss: 13799.3301\n",
      "Epoch 71/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5263.2881\n",
      "Epoch 71: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 4469.9214 - val_loss: 14385.3594\n",
      "Epoch 72/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3395.0972\n",
      "Epoch 72: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 4361.4722 - val_loss: 14085.4033\n",
      "Epoch 73/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5332.1016\n",
      "Epoch 73: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 4242.1914 - val_loss: 13128.4766\n",
      "Epoch 74/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4938.7993\n",
      "Epoch 74: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 4141.7222 - val_loss: 13868.1113\n",
      "Epoch 75/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5284.7505\n",
      "Epoch 75: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 4199.1924 - val_loss: 13696.5703\n",
      "Epoch 76/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3956.7261\n",
      "Epoch 76: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 3988.2671 - val_loss: 12502.0645\n",
      "Epoch 77/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2657.1023\n",
      "Epoch 77: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 3883.4082 - val_loss: 13910.2871\n",
      "Epoch 78/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4565.8032\n",
      "Epoch 78: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 3898.2966 - val_loss: 13190.5957\n",
      "Epoch 79/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4450.6128\n",
      "Epoch 79: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4094.1926 - val_loss: 12319.2734\n",
      "Epoch 80/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3971.6907\n",
      "Epoch 80: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 3775.3450 - val_loss: 13693.6562\n",
      "Epoch 81/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2105.4888\n",
      "Epoch 81: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 3551.9495 - val_loss: 11986.1035\n",
      "Epoch 82/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3863.0095\n",
      "Epoch 82: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 3674.7002 - val_loss: 12349.0986\n",
      "Epoch 83/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1558.6108\n",
      "Epoch 83: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 3278.1650 - val_loss: 14265.3799\n",
      "Epoch 84/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2621.8772\n",
      "Epoch 84: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 3523.3333 - val_loss: 11746.0283\n",
      "Epoch 85/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4131.8770\n",
      "Epoch 85: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 3390.3965 - val_loss: 11461.3945\n",
      "Epoch 86/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3723.4487\n",
      "Epoch 86: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 3101.6311 - val_loss: 13316.8643\n",
      "Epoch 87/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3677.1470\n",
      "Epoch 87: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 3424.7466 - val_loss: 11056.5156\n",
      "Epoch 88/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1926.8245\n",
      "Epoch 88: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 3254.8232 - val_loss: 10868.8867\n",
      "Epoch 89/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3576.6721\n",
      "Epoch 89: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 3013.6101 - val_loss: 14518.2168\n",
      "Epoch 90/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4472.9561\n",
      "Epoch 90: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 3565.7146 - val_loss: 11493.2129\n",
      "Epoch 91/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2014.5433\n",
      "Epoch 91: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 2866.0342 - val_loss: 10394.1162\n",
      "Epoch 92/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2248.8794\n",
      "Epoch 92: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 3134.0525 - val_loss: 12707.8057\n",
      "Epoch 93/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3525.2566\n",
      "Epoch 93: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2953.7124 - val_loss: 11016.0205\n",
      "Epoch 94/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3040.0691\n",
      "Epoch 94: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 2800.7092 - val_loss: 10579.0635\n",
      "Epoch 95/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2964.2407\n",
      "Epoch 95: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2552.0571 - val_loss: 12404.2158\n",
      "Epoch 96/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3730.2178\n",
      "Epoch 96: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 2916.8833 - val_loss: 10242.7607\n",
      "Epoch 97/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2980.1091\n",
      "Epoch 97: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2748.6572 - val_loss: 9969.5967\n",
      "Epoch 98/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1507.4841\n",
      "Epoch 98: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 2683.1831 - val_loss: 11980.3096\n",
      "Epoch 99/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1396.2512\n",
      "Epoch 99: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 2595.7559 - val_loss: 9554.2500\n",
      "Epoch 100/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2100.5576\n",
      "Epoch 100: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2592.5657 - val_loss: 9681.7129\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9681.7129\n",
      "Epoch 1/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 793401.3125\n",
      "Epoch 1: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 1s 149ms/step - loss: 762625.3125 - val_loss: 491427.8438\n",
      "Epoch 2/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 729466.1875\n",
      "Epoch 2: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 548721.3125 - val_loss: 99315.4531\n",
      "Epoch 3/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 144320.7969\n",
      "Epoch 3: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 341278.6250 - val_loss: 219424.9062\n",
      "Epoch 4/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 293889.2812\n",
      "Epoch 4: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 205771.2344 - val_loss: 146080.1719\n",
      "Epoch 5/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 288918.1562\n",
      "Epoch 5: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 264120.0938 - val_loss: 248834.5312\n",
      "Epoch 6/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 316206.1250\n",
      "Epoch 6: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 357136.0000 - val_loss: 205960.8750\n",
      "Epoch 7/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 343620.2812\n",
      "Epoch 7: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 286529.2812 - val_loss: 74379.9062\n",
      "Epoch 8/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 160000.4375\n",
      "Epoch 8: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 149125.9844 - val_loss: 166862.4375\n",
      "Epoch 9/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 194574.7031\n",
      "Epoch 9: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 214184.8438 - val_loss: 100641.5547\n",
      "Epoch 10/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 183825.9688\n",
      "Epoch 10: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 138204.3906 - val_loss: 48149.1719\n",
      "Epoch 11/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 122814.5234\n",
      "Epoch 11: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 131680.5469 - val_loss: 74745.5938\n",
      "Epoch 12/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 147107.6094\n",
      "Epoch 12: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 159110.0156 - val_loss: 52173.2852\n",
      "Epoch 13/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 133519.0312\n",
      "Epoch 13: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 123211.7812 - val_loss: 28640.7383\n",
      "Epoch 14/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 101253.4297\n",
      "Epoch 14: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 105411.7969 - val_loss: 61352.2578\n",
      "Epoch 15/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 115471.3438\n",
      "Epoch 15: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 114945.8359 - val_loss: 24446.1445\n",
      "Epoch 16/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 110100.3672\n",
      "Epoch 16: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 80904.0781 - val_loss: 14619.5498\n",
      "Epoch 17/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 82290.2969\n",
      "Epoch 17: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 84129.1641 - val_loss: 21151.2246\n",
      "Epoch 18/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 72317.9297\n",
      "Epoch 18: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 83274.5078 - val_loss: 6740.5674\n",
      "Epoch 19/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 83535.1719\n",
      "Epoch 19: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 65704.4531 - val_loss: 13457.4531\n",
      "Epoch 20/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 62812.4766\n",
      "Epoch 20: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 61838.0898 - val_loss: 11713.0723\n",
      "Epoch 21/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 62683.4961\n",
      "Epoch 21: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 54002.4688 - val_loss: 2726.0178\n",
      "Epoch 22/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 24865.2539\n",
      "Epoch 22: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 43916.6758 - val_loss: 6444.5332\n",
      "Epoch 23/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 60168.8008\n",
      "Epoch 23: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 43751.1602 - val_loss: 5030.1685\n",
      "Epoch 24/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 42594.0039\n",
      "Epoch 24: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 35569.8984 - val_loss: 7242.9385\n",
      "Epoch 25/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 34741.6055\n",
      "Epoch 25: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 31963.2832 - val_loss: 8908.8652\n",
      "Epoch 26/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 27800.6875\n",
      "Epoch 26: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 28454.1914 - val_loss: 9076.0850\n",
      "Epoch 27/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 23495.9824\n",
      "Epoch 27: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 27048.9590 - val_loss: 11218.8936\n",
      "Epoch 28/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 24299.5391\n",
      "Epoch 28: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 25834.2910 - val_loss: 9732.1006\n",
      "Epoch 29/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 19194.7539\n",
      "Epoch 29: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 22273.9141 - val_loss: 9659.7842\n",
      "Epoch 30/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 24946.6562\n",
      "Epoch 30: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 21113.1367 - val_loss: 8376.1807\n",
      "Epoch 31/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 17405.8477\n",
      "Epoch 31: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 19702.7676 - val_loss: 8287.8213\n",
      "Epoch 32/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 23022.3633\n",
      "Epoch 32: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 17939.9551 - val_loss: 6587.2939\n",
      "Epoch 33/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 17604.7656\n",
      "Epoch 33: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 17233.4668 - val_loss: 5987.7876\n",
      "Epoch 34/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 17232.9121\n",
      "Epoch 34: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 15660.6494 - val_loss: 6134.5635\n",
      "Epoch 35/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 16532.2148\n",
      "Epoch 35: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 14994.1152 - val_loss: 5274.6509\n",
      "Epoch 36/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 16832.2754\n",
      "Epoch 36: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 14297.8477 - val_loss: 4174.4751\n",
      "Epoch 37/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 15762.9834\n",
      "Epoch 37: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 13213.2832 - val_loss: 3898.6570\n",
      "Epoch 38/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9645.0928\n",
      "Epoch 38: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 12562.8047 - val_loss: 4191.3086\n",
      "Epoch 39/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 14777.2598\n",
      "Epoch 39: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 11846.5352 - val_loss: 3364.1094\n",
      "Epoch 40/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7082.5913\n",
      "Epoch 40: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 12204.9004 - val_loss: 3218.5613\n",
      "Epoch 41/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 14789.3701\n",
      "Epoch 41: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 11016.1992 - val_loss: 5342.4810\n",
      "Epoch 42/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10405.9707\n",
      "Epoch 42: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 11155.6895 - val_loss: 3962.9973\n",
      "Epoch 43/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 12079.1064\n",
      "Epoch 43: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 9517.4326 - val_loss: 3240.7297\n",
      "Epoch 44/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8890.1729\n",
      "Epoch 44: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 10978.6738 - val_loss: 3078.3235\n",
      "Epoch 45/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10691.9229\n",
      "Epoch 45: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 9337.4258 - val_loss: 6811.9717\n",
      "Epoch 46/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 13580.3525\n",
      "Epoch 46: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 9999.2754 - val_loss: 4145.9146\n",
      "Epoch 47/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10554.3311\n",
      "Epoch 47: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 8661.6523 - val_loss: 2664.8423\n",
      "Epoch 48/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9180.2412\n",
      "Epoch 48: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 9097.7041 - val_loss: 3030.5002\n",
      "Epoch 49/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9771.3545\n",
      "Epoch 49: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 8016.1211 - val_loss: 3468.9031\n",
      "Epoch 50/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7480.1240\n",
      "Epoch 50: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 7674.9194 - val_loss: 3093.0540\n",
      "Epoch 51/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9258.9160\n",
      "Epoch 51: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 7359.1782 - val_loss: 2841.8796\n",
      "Epoch 52/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6836.3862\n",
      "Epoch 52: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 7150.1943 - val_loss: 2767.1824\n",
      "Epoch 53/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7379.5522\n",
      "Epoch 53: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6970.7651 - val_loss: 3199.3108\n",
      "Epoch 54/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8827.3760\n",
      "Epoch 54: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6836.6924 - val_loss: 2654.1399\n",
      "Epoch 55/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8370.9814\n",
      "Epoch 55: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 6581.1118 - val_loss: 2166.9160\n",
      "Epoch 56/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5319.1348\n",
      "Epoch 56: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 6494.3574 - val_loss: 3357.0962\n",
      "Epoch 57/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6696.6582\n",
      "Epoch 57: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 6638.6968 - val_loss: 3809.4331\n",
      "Epoch 58/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7792.5938\n",
      "Epoch 58: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 6248.7476 - val_loss: 2225.7776\n",
      "Epoch 59/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5519.0542\n",
      "Epoch 59: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 6245.9292 - val_loss: 3250.0461\n",
      "Epoch 60/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3791.0381\n",
      "Epoch 60: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 6080.0952 - val_loss: 4309.3286\n",
      "Epoch 61/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5146.6489\n",
      "Epoch 61: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 6021.8682 - val_loss: 2611.2795\n",
      "Epoch 62/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6354.7515\n",
      "Epoch 62: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 5485.3354 - val_loss: 3382.8196\n",
      "Epoch 63/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3440.0500\n",
      "Epoch 63: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 5297.6533 - val_loss: 3034.4592\n",
      "Epoch 64/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4369.8696\n",
      "Epoch 64: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 5115.7744 - val_loss: 2546.7559\n",
      "Epoch 65/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5696.8618\n",
      "Epoch 65: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4985.4902 - val_loss: 3184.6602\n",
      "Epoch 66/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3513.3113\n",
      "Epoch 66: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 4817.6055 - val_loss: 3181.9226\n",
      "Epoch 67/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6317.4185\n",
      "Epoch 67: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 4731.0234 - val_loss: 2735.6677\n",
      "Epoch 68/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5733.1006\n",
      "Epoch 68: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4619.2466 - val_loss: 2671.5793\n",
      "Epoch 69/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4922.3745\n",
      "Epoch 69: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 4446.8916 - val_loss: 3591.7163\n",
      "Epoch 70/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2939.7065\n",
      "Epoch 70: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 4416.6523 - val_loss: 3488.3040\n",
      "Epoch 71/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4980.3867\n",
      "Epoch 71: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 4262.5049 - val_loss: 2665.3879\n",
      "Epoch 72/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3251.6025\n",
      "Epoch 72: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 4173.8369 - val_loss: 2815.5049\n",
      "Epoch 73/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5066.7588\n",
      "Epoch 73: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 4066.3762 - val_loss: 3528.8718\n",
      "Epoch 74/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4707.4985\n",
      "Epoch 74: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 3954.2383 - val_loss: 2488.7505\n",
      "Epoch 75/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5060.3965\n",
      "Epoch 75: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 4045.7197 - val_loss: 2780.6768\n",
      "Epoch 76/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3714.5425\n",
      "Epoch 76: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 3852.0637 - val_loss: 3974.7781\n",
      "Epoch 77/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2567.6267\n",
      "Epoch 77: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 3704.1711 - val_loss: 2152.2717\n",
      "Epoch 78/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4406.3599\n",
      "Epoch 78: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 3789.3354 - val_loss: 2781.5005\n",
      "Epoch 79/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4092.7307\n",
      "Epoch 79: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 3990.0334 - val_loss: 3358.5632\n",
      "Epoch 80/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3780.1296\n",
      "Epoch 80: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 3697.1929 - val_loss: 1997.9568\n",
      "Epoch 81/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2073.7192\n",
      "Epoch 81: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 3414.4102 - val_loss: 4202.3931\n",
      "Epoch 82/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3838.8416\n",
      "Epoch 82: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3671.7341 - val_loss: 2837.1992\n",
      "Epoch 83/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1534.0929\n",
      "Epoch 83: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 3159.7632 - val_loss: 1739.2002\n",
      "Epoch 84/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2618.9114\n",
      "Epoch 84: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 3366.1099 - val_loss: 4065.2710\n",
      "Epoch 85/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4016.1145\n",
      "Epoch 85: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 3356.8359 - val_loss: 3075.8877\n",
      "Epoch 86/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3534.9041\n",
      "Epoch 86: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2963.5054 - val_loss: 1867.7100\n",
      "Epoch 87/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3572.0723\n",
      "Epoch 87: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 3196.3354 - val_loss: 4293.6943\n",
      "Epoch 88/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2085.6897\n",
      "Epoch 88: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 3258.2488 - val_loss: 3445.8567\n",
      "Epoch 89/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3320.5974\n",
      "Epoch 89: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 2740.8118 - val_loss: 1549.5100\n",
      "Epoch 90/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4238.6553\n",
      "Epoch 90: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 3256.0381 - val_loss: 3629.0327\n",
      "Epoch 91/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2219.4089\n",
      "Epoch 91: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2902.3145 - val_loss: 3291.5103\n",
      "Epoch 92/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1787.1718\n",
      "Epoch 92: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2766.4548 - val_loss: 1751.9164\n",
      "Epoch 93/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3240.8018\n",
      "Epoch 93: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2722.1086 - val_loss: 3010.5188\n",
      "Epoch 94/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2886.8130\n",
      "Epoch 94: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2579.9094 - val_loss: 2140.8821\n",
      "Epoch 95/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2960.9658\n",
      "Epoch 95: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 2482.5774 - val_loss: 1729.6301\n",
      "Epoch 96/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3355.1812\n",
      "Epoch 96: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2550.9023 - val_loss: 3237.5273\n",
      "Epoch 97/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2867.5554\n",
      "Epoch 97: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2583.0300 - val_loss: 2461.9700\n",
      "Epoch 98/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1256.2983\n",
      "Epoch 98: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 2444.8059 - val_loss: 1667.4111\n",
      "Epoch 99/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1198.9512\n",
      "Epoch 99: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 2332.3105 - val_loss: 3400.8940\n",
      "Epoch 100/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1944.1554\n",
      "Epoch 100: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2422.7190 - val_loss: 2296.5076\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2296.5076\n",
      "Epoch 1/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 793376.3125\n",
      "Epoch 1: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 765690.9375 - val_loss: 934865.1875\n",
      "Epoch 2/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 767469.1250\n",
      "Epoch 2: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 595836.6250 - val_loss: 387888.0625\n",
      "Epoch 3/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 182692.0312\n",
      "Epoch 3: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 284558.5938 - val_loss: 342686.3438\n",
      "Epoch 4/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 294200.5938\n",
      "Epoch 4: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 208331.9375 - val_loss: 383315.3750\n",
      "Epoch 5/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 235885.3125\n",
      "Epoch 5: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 208141.6406 - val_loss: 471220.0938\n",
      "Epoch 6/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 225184.2656\n",
      "Epoch 6: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 254089.0781 - val_loss: 365648.6250\n",
      "Epoch 7/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 210291.7344\n",
      "Epoch 7: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 172339.2500 - val_loss: 254375.7812\n",
      "Epoch 8/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 159374.3906\n",
      "Epoch 8: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 162875.8750 - val_loss: 243347.0938\n",
      "Epoch 9/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 133850.2344\n",
      "Epoch 9: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 145731.7031 - val_loss: 252358.2188\n",
      "Epoch 10/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 160706.3750\n",
      "Epoch 10: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 120104.9922 - val_loss: 276526.1875\n",
      "Epoch 11/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 129676.5859\n",
      "Epoch 11: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 128405.9297 - val_loss: 234507.5469\n",
      "Epoch 12/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 99276.8594\n",
      "Epoch 12: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 103203.9609 - val_loss: 183476.1406\n",
      "Epoch 13/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 102635.0703\n",
      "Epoch 13: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 96796.4141 - val_loss: 165826.4219\n",
      "Epoch 14/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 97521.2969\n",
      "Epoch 14: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 94385.2031 - val_loss: 159126.5938\n",
      "Epoch 15/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 66922.1328\n",
      "Epoch 15: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 79261.7500 - val_loss: 176799.7500\n",
      "Epoch 16/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 113077.9453\n",
      "Epoch 16: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 80080.6328 - val_loss: 143305.1875\n",
      "Epoch 17/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 66825.9844\n",
      "Epoch 17: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 62907.3281 - val_loss: 103631.3906\n",
      "Epoch 18/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 51103.4844\n",
      "Epoch 18: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 54175.4961 - val_loss: 85192.6562\n",
      "Epoch 19/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 61118.9375\n",
      "Epoch 19: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 50427.7070 - val_loss: 76304.5312\n",
      "Epoch 20/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 39404.8750\n",
      "Epoch 20: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 43083.2305 - val_loss: 75694.8594\n",
      "Epoch 21/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 42376.5820\n",
      "Epoch 21: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 39062.3672 - val_loss: 48004.4688\n",
      "Epoch 22/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 17460.6035\n",
      "Epoch 22: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 31118.5059 - val_loss: 40087.8438\n",
      "Epoch 23/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 37171.6719\n",
      "Epoch 23: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 31548.1543 - val_loss: 31392.9121\n",
      "Epoch 24/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 25703.8066\n",
      "Epoch 24: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 25525.3301 - val_loss: 32953.1523\n",
      "Epoch 25/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 29635.2090\n",
      "Epoch 25: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 26786.9766 - val_loss: 19720.2090\n",
      "Epoch 26/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 19455.6074\n",
      "Epoch 26: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 21661.9062 - val_loss: 20238.0977\n",
      "Epoch 27/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 24696.0312\n",
      "Epoch 27: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 21774.3125 - val_loss: 13994.9297\n",
      "Epoch 28/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 13971.9795\n",
      "Epoch 28: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 17281.8809 - val_loss: 12501.6826\n",
      "Epoch 29/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 15082.9062\n",
      "Epoch 29: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 17055.9590 - val_loss: 8508.6895\n",
      "Epoch 30/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 17088.2598\n",
      "Epoch 30: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 15174.1738 - val_loss: 9125.0508\n",
      "Epoch 31/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 14014.6807\n",
      "Epoch 31: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 15089.7148 - val_loss: 5382.7290\n",
      "Epoch 32/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 16610.6309\n",
      "Epoch 32: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 13100.6211 - val_loss: 4956.8354\n",
      "Epoch 33/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 13569.5693\n",
      "Epoch 33: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 12426.3584 - val_loss: 4173.1543\n",
      "Epoch 34/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 12635.2852\n",
      "Epoch 34: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 11469.3047 - val_loss: 3786.6301\n",
      "Epoch 35/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 11976.7832\n",
      "Epoch 35: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 10948.1543 - val_loss: 3635.4734\n",
      "Epoch 36/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 13022.2383\n",
      "Epoch 36: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 10449.3613 - val_loss: 4172.2148\n",
      "Epoch 37/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 12731.5449\n",
      "Epoch 37: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 10490.9277 - val_loss: 3129.3574\n",
      "Epoch 38/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7665.4424\n",
      "Epoch 38: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 9551.0137 - val_loss: 2915.1777\n",
      "Epoch 39/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 12065.6621\n",
      "Epoch 39: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 9504.9756 - val_loss: 2728.5181\n",
      "Epoch 40/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5425.7251\n",
      "Epoch 40: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 8989.7109 - val_loss: 3132.4194\n",
      "Epoch 41/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 11293.7197\n",
      "Epoch 41: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 8779.4238 - val_loss: 3612.3198\n",
      "Epoch 42/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8486.0654\n",
      "Epoch 42: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 8242.6904 - val_loss: 2736.2444\n",
      "Epoch 43/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10583.5527\n",
      "Epoch 43: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 8210.0312 - val_loss: 2864.4863\n",
      "Epoch 44/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6113.1914\n",
      "Epoch 44: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 8051.9849 - val_loss: 3013.2761\n",
      "Epoch 45/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8536.7363\n",
      "Epoch 45: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 7486.5322 - val_loss: 3499.6343\n",
      "Epoch 46/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10079.8389\n",
      "Epoch 46: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 7315.7095 - val_loss: 2620.6582\n",
      "Epoch 47/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8470.7041\n",
      "Epoch 47: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 7001.5693 - val_loss: 2655.0317\n",
      "Epoch 48/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6946.3550\n",
      "Epoch 48: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 7161.1958 - val_loss: 2793.4104\n",
      "Epoch 49/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7563.8848\n",
      "Epoch 49: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 6394.0288 - val_loss: 2838.0928\n",
      "Epoch 50/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5578.5488\n",
      "Epoch 50: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 6553.4800 - val_loss: 2881.0083\n",
      "Epoch 51/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7440.8506\n",
      "Epoch 51: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 6288.1167 - val_loss: 3363.1453\n",
      "Epoch 52/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5376.6509\n",
      "Epoch 52: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 6166.1904 - val_loss: 2503.3113\n",
      "Epoch 53/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6433.5977\n",
      "Epoch 53: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5919.3306 - val_loss: 2664.5042\n",
      "Epoch 54/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7279.2109\n",
      "Epoch 54: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 5791.0718 - val_loss: 2879.7595\n",
      "Epoch 55/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6876.7583\n",
      "Epoch 55: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 5434.3232 - val_loss: 2386.2861\n",
      "Epoch 56/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3736.6948\n",
      "Epoch 56: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 5561.9634 - val_loss: 2858.0317\n",
      "Epoch 57/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6043.3442\n",
      "Epoch 57: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 5698.2979 - val_loss: 3023.0959\n",
      "Epoch 58/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6302.0635\n",
      "Epoch 58: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 5228.6724 - val_loss: 2823.4348\n",
      "Epoch 59/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4286.4565\n",
      "Epoch 59: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5484.1763 - val_loss: 2969.1060\n",
      "Epoch 60/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3287.0640\n",
      "Epoch 60: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 5443.5000 - val_loss: 2792.4299\n",
      "Epoch 61/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3766.7009\n",
      "Epoch 61: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 5391.6479 - val_loss: 1928.2279\n",
      "Epoch 62/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5732.6924\n",
      "Epoch 62: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 4682.9790 - val_loss: 3243.6350\n",
      "Epoch 63/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3304.2739\n",
      "Epoch 63: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 4707.9648 - val_loss: 1862.7936\n",
      "Epoch 64/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3099.0051\n",
      "Epoch 64: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 4376.7422 - val_loss: 1794.4314\n",
      "Epoch 65/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5373.4429\n",
      "Epoch 65: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 4192.7593 - val_loss: 2904.5947\n",
      "Epoch 66/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3378.4526\n",
      "Epoch 66: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 4316.9473 - val_loss: 1956.2590\n",
      "Epoch 67/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5233.7700\n",
      "Epoch 67: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 3948.8525 - val_loss: 1878.3552\n",
      "Epoch 68/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4605.0093\n",
      "Epoch 68: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 3797.9985 - val_loss: 1925.6360\n",
      "Epoch 69/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4136.1646\n",
      "Epoch 69: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 3714.6685 - val_loss: 1899.4873\n",
      "Epoch 70/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2063.8484\n",
      "Epoch 70: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3689.1890 - val_loss: 1979.7971\n",
      "Epoch 71/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4537.9321\n",
      "Epoch 71: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 3570.2307 - val_loss: 1855.6947\n",
      "Epoch 72/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2451.3616\n",
      "Epoch 72: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 3493.1877 - val_loss: 1718.2386\n",
      "Epoch 73/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4253.1167\n",
      "Epoch 73: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 3484.7339 - val_loss: 1793.9333\n",
      "Epoch 74/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4299.8843\n",
      "Epoch 74: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 3325.4880 - val_loss: 1442.3152\n",
      "Epoch 75/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4255.0146\n",
      "Epoch 75: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3345.3462 - val_loss: 1846.4470\n",
      "Epoch 76/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3644.5376\n",
      "Epoch 76: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 3348.7222 - val_loss: 1667.1658\n",
      "Epoch 77/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1981.6818\n",
      "Epoch 77: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3190.6360 - val_loss: 1445.6368\n",
      "Epoch 78/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3865.6138\n",
      "Epoch 78: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 3137.8630 - val_loss: 2191.6775\n",
      "Epoch 79/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3356.1301\n",
      "Epoch 79: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 3380.8247 - val_loss: 1371.0688\n",
      "Epoch 80/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3489.5728\n",
      "Epoch 80: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 3663.2783 - val_loss: 1319.6569\n",
      "Epoch 81/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1708.2487\n",
      "Epoch 81: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 2851.2515 - val_loss: 3948.3875\n",
      "Epoch 82/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4331.7935\n",
      "Epoch 82: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 3528.3186 - val_loss: 1503.6322\n",
      "Epoch 83/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1983.9409\n",
      "Epoch 83: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 3338.0586 - val_loss: 1779.8040\n",
      "Epoch 84/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1635.2817\n",
      "Epoch 84: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2813.7507 - val_loss: 2488.4543\n",
      "Epoch 85/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3809.2161\n",
      "Epoch 85: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2829.0042 - val_loss: 1295.2783\n",
      "Epoch 86/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3830.9150\n",
      "Epoch 86: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 3035.9473 - val_loss: 1626.9084\n",
      "Epoch 87/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3201.9731\n",
      "Epoch 87: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2648.5000 - val_loss: 1584.0450\n",
      "Epoch 88/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1229.3551\n",
      "Epoch 88: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 2654.0972 - val_loss: 1366.4535\n",
      "Epoch 89/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3047.6841\n",
      "Epoch 89: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 2626.3132 - val_loss: 1478.1312\n",
      "Epoch 90/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3081.2517\n",
      "Epoch 90: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 2426.6033 - val_loss: 1193.5837\n",
      "Epoch 91/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1588.3931\n",
      "Epoch 91: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2405.2078 - val_loss: 1954.6858\n",
      "Epoch 92/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1506.0930\n",
      "Epoch 92: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2537.4604 - val_loss: 1180.8573\n",
      "Epoch 93/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2745.1357\n",
      "Epoch 93: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 2359.0203 - val_loss: 1202.9756\n",
      "Epoch 94/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2786.4949\n",
      "Epoch 94: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 2371.4092 - val_loss: 1411.5123\n",
      "Epoch 95/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2684.8801\n",
      "Epoch 95: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 2177.3044 - val_loss: 1334.9174\n",
      "Epoch 96/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3206.5210\n",
      "Epoch 96: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 2421.9824 - val_loss: 1749.6949\n",
      "Epoch 97/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2759.6946\n",
      "Epoch 97: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 2384.0312 - val_loss: 1224.5941\n",
      "Epoch 98/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1122.6498\n",
      "Epoch 98: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2386.1091 - val_loss: 1097.0900\n",
      "Epoch 99/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 850.5504\n",
      "Epoch 99: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 2121.5505 - val_loss: 1652.0209\n",
      "Epoch 100/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1771.0242\n",
      "Epoch 100: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 2384.5203 - val_loss: 1095.1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1095.1000\n",
      "Epoch 1/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 793351.8750\n",
      "Epoch 1: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 1s 126ms/step - loss: 764929.0625 - val_loss: 1158341.2500\n",
      "Epoch 2/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 758350.0000\n",
      "Epoch 2: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 584246.8125 - val_loss: 441075.3125\n",
      "Epoch 3/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 166021.8594\n",
      "Epoch 3: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 309637.9375 - val_loss: 284851.6562\n",
      "Epoch 4/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 301114.5000\n",
      "Epoch 4: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 211595.9219 - val_loss: 519883.5938\n",
      "Epoch 5/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 260304.7031\n",
      "Epoch 5: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 233087.1875 - val_loss: 665171.4375\n",
      "Epoch 6/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 262758.4375\n",
      "Epoch 6: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 296333.6875 - val_loss: 544701.1875\n",
      "Epoch 7/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 260383.2031\n",
      "Epoch 7: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 211175.7500 - val_loss: 283482.7188\n",
      "Epoch 8/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 139848.2031\n",
      "Epoch 8: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 150157.4375 - val_loss: 233199.7969\n",
      "Epoch 9/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 180770.0781\n",
      "Epoch 9: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 185114.0000 - val_loss: 255240.3281\n",
      "Epoch 10/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 156658.7031\n",
      "Epoch 10: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 117064.0781 - val_loss: 349763.2188\n",
      "Epoch 11/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 135128.2969\n",
      "Epoch 11: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 138362.2188 - val_loss: 354315.8750\n",
      "Epoch 12/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 125983.8203\n",
      "Epoch 12: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 131445.9375 - val_loss: 256138.4531\n",
      "Epoch 13/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 104441.9062\n",
      "Epoch 13: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 97797.7500 - val_loss: 178996.7188\n",
      "Epoch 14/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 104484.5156\n",
      "Epoch 14: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 111487.4453 - val_loss: 164857.0938\n",
      "Epoch 15/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 91986.5625\n",
      "Epoch 15: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 93961.9531 - val_loss: 199994.6094\n",
      "Epoch 16/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 108455.7812\n",
      "Epoch 16: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 78025.4453 - val_loss: 221396.5781\n",
      "Epoch 17/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 84242.2266\n",
      "Epoch 17: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 82455.0625 - val_loss: 176092.5000\n",
      "Epoch 18/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 57024.4453\n",
      "Epoch 18: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 62711.6055 - val_loss: 111173.5469\n",
      "Epoch 19/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 70658.0859\n",
      "Epoch 19: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 63243.7773 - val_loss: 89184.0156\n",
      "Epoch 20/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 59139.8750\n",
      "Epoch 20: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 54718.8086 - val_loss: 96531.5781\n",
      "Epoch 21/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 47297.4961\n",
      "Epoch 21: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 45131.5977 - val_loss: 94997.7109\n",
      "Epoch 22/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 25939.1738\n",
      "Epoch 22: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 40611.3906 - val_loss: 55593.0547\n",
      "Epoch 23/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 43084.1055\n",
      "Epoch 23: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 35641.3398 - val_loss: 33047.3594\n",
      "Epoch 24/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 37054.0977\n",
      "Epoch 24: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 32783.3672 - val_loss: 30192.1055\n",
      "Epoch 25/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 29268.8359\n",
      "Epoch 25: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 27011.8340 - val_loss: 33860.3633\n",
      "Epoch 26/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 28283.2520\n",
      "Epoch 26: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 26748.3281 - val_loss: 18970.5684\n",
      "Epoch 27/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 21927.7363\n",
      "Epoch 27: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 21784.9648 - val_loss: 9344.9131\n",
      "Epoch 28/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 17567.1504\n",
      "Epoch 28: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 22668.6660 - val_loss: 7813.5596\n",
      "Epoch 29/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 17376.5117\n",
      "Epoch 29: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 19752.8086 - val_loss: 15174.4717\n",
      "Epoch 30/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 22496.7988\n",
      "Epoch 30: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 18901.1914 - val_loss: 10690.8506\n",
      "Epoch 31/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 14381.6592\n",
      "Epoch 31: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 16159.7139 - val_loss: 5219.6768\n",
      "Epoch 32/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 20210.1094\n",
      "Epoch 32: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 15746.3232 - val_loss: 5061.6006\n",
      "Epoch 33/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 15152.8906\n",
      "Epoch 33: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 14135.5117 - val_loss: 10458.6924\n",
      "Epoch 34/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 14509.0566\n",
      "Epoch 34: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 13790.0352 - val_loss: 12129.7236\n",
      "Epoch 35/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 14656.9600\n",
      "Epoch 35: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 13000.6973 - val_loss: 6489.3169\n",
      "Epoch 36/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 14027.6299\n",
      "Epoch 36: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 12479.4082 - val_loss: 6932.1006\n",
      "Epoch 37/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 13460.5791\n",
      "Epoch 37: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 11305.6953 - val_loss: 12346.1162\n",
      "Epoch 38/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8814.7305\n",
      "Epoch 38: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 11775.8438 - val_loss: 10324.8906\n",
      "Epoch 39/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 12867.6240\n",
      "Epoch 39: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 10180.2510 - val_loss: 6730.9883\n",
      "Epoch 40/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6530.3770\n",
      "Epoch 40: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 11937.5215 - val_loss: 7982.2251\n",
      "Epoch 41/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 12786.9424\n",
      "Epoch 41: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 10037.6484 - val_loss: 16023.6309\n",
      "Epoch 42/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10856.7549\n",
      "Epoch 42: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 11369.6094 - val_loss: 7685.9170\n",
      "Epoch 43/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10897.5059\n",
      "Epoch 43: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 8540.6523 - val_loss: 7493.0640\n",
      "Epoch 44/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10065.1631\n",
      "Epoch 44: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 11436.2070 - val_loss: 7680.1436\n",
      "Epoch 45/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9359.6816\n",
      "Epoch 45: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 8707.0938 - val_loss: 13813.2783\n",
      "Epoch 46/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 13806.5215\n",
      "Epoch 46: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 9960.8486 - val_loss: 6045.2041\n",
      "Epoch 47/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9451.0713\n",
      "Epoch 47: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 8170.1997 - val_loss: 5272.3242\n",
      "Epoch 48/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9434.5215\n",
      "Epoch 48: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 8897.1416 - val_loss: 6508.7056\n",
      "Epoch 49/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8829.8193\n",
      "Epoch 49: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 7481.9087 - val_loss: 5808.2007\n",
      "Epoch 50/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6495.6250\n",
      "Epoch 50: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 7165.5352 - val_loss: 4612.6016\n",
      "Epoch 51/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8778.5352\n",
      "Epoch 51: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 6951.4702 - val_loss: 5364.2271\n",
      "Epoch 52/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6006.0503\n",
      "Epoch 52: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 6591.7505 - val_loss: 5545.5859\n",
      "Epoch 53/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6971.7793\n",
      "Epoch 53: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 6518.1528 - val_loss: 4785.8174\n",
      "Epoch 54/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8179.9526\n",
      "Epoch 54: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 6217.4995 - val_loss: 4083.3528\n",
      "Epoch 55/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7828.3120\n",
      "Epoch 55: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 6148.7236 - val_loss: 3903.0728\n",
      "Epoch 56/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4641.6289\n",
      "Epoch 56: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 5950.6035 - val_loss: 4735.0737\n",
      "Epoch 57/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6620.6982\n",
      "Epoch 57: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 6105.6162 - val_loss: 4227.3833\n",
      "Epoch 58/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7032.9204\n",
      "Epoch 58: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 5793.5557 - val_loss: 3869.1843\n",
      "Epoch 59/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4801.2334\n",
      "Epoch 59: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 5738.6943 - val_loss: 5088.1846\n",
      "Epoch 60/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3660.1262\n",
      "Epoch 60: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 5777.6357 - val_loss: 4895.8760\n",
      "Epoch 61/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4401.3481\n",
      "Epoch 61: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 5838.0625 - val_loss: 3899.0513\n",
      "Epoch 62/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6187.6431\n",
      "Epoch 62: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5239.3721 - val_loss: 5048.4966\n",
      "Epoch 63/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3409.7817\n",
      "Epoch 63: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 5201.9824 - val_loss: 3859.7559\n",
      "Epoch 64/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3862.8503\n",
      "Epoch 64: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 4932.2153 - val_loss: 3799.1270\n",
      "Epoch 65/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5828.6445\n",
      "Epoch 65: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 4809.7759 - val_loss: 5184.6084\n",
      "Epoch 66/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3572.9858\n",
      "Epoch 66: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 4679.6172 - val_loss: 5088.2271\n",
      "Epoch 67/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6080.1890\n",
      "Epoch 67: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 4530.3022 - val_loss: 4550.7217\n",
      "Epoch 68/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5505.3926\n",
      "Epoch 68: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 4478.4263 - val_loss: 4428.3223\n",
      "Epoch 69/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4691.1479\n",
      "Epoch 69: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 4231.3027 - val_loss: 4679.2144\n",
      "Epoch 70/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2705.1106\n",
      "Epoch 70: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 4290.4824 - val_loss: 3978.3347\n",
      "Epoch 71/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4969.2012\n",
      "Epoch 71: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 4089.6074 - val_loss: 3832.4714\n",
      "Epoch 72/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2949.4255\n",
      "Epoch 72: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 3980.3447 - val_loss: 4034.9421\n",
      "Epoch 73/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4888.0020\n",
      "Epoch 73: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 3930.3018 - val_loss: 4147.3438\n",
      "Epoch 74/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4753.3418\n",
      "Epoch 74: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 3795.6555 - val_loss: 3655.6030\n",
      "Epoch 75/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4886.4160\n",
      "Epoch 75: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 3800.9536 - val_loss: 3898.9036\n",
      "Epoch 76/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3909.4028\n",
      "Epoch 76: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 3731.6951 - val_loss: 4118.5317\n",
      "Epoch 77/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2389.3340\n",
      "Epoch 77: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 3556.6135 - val_loss: 3508.6328\n",
      "Epoch 78/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4306.0698\n",
      "Epoch 78: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 3628.2986 - val_loss: 4127.6841\n",
      "Epoch 79/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3873.6074\n",
      "Epoch 79: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 3863.2639 - val_loss: 3632.6616\n",
      "Epoch 80/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3661.0278\n",
      "Epoch 80: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 3654.0950 - val_loss: 2832.5293\n",
      "Epoch 81/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2102.3418\n",
      "Epoch 81: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 3339.2520 - val_loss: 4905.7412\n",
      "Epoch 82/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4216.3828\n",
      "Epoch 82: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 3795.7083 - val_loss: 2730.7119\n",
      "Epoch 83/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1539.5052\n",
      "Epoch 83: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 3193.1160 - val_loss: 2616.7505\n",
      "Epoch 84/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2307.0393\n",
      "Epoch 84: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 3285.2319 - val_loss: 4309.5381\n",
      "Epoch 85/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4215.1206\n",
      "Epoch 85: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 3412.2983 - val_loss: 2693.2334\n",
      "Epoch 86/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3645.0752\n",
      "Epoch 86: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 3030.3201 - val_loss: 2422.6584\n",
      "Epoch 87/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3539.2839\n",
      "Epoch 87: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 2954.5649 - val_loss: 3692.8716\n",
      "Epoch 88/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1981.2704\n",
      "Epoch 88: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 3171.3567 - val_loss: 2310.6479\n",
      "Epoch 89/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3275.5232\n",
      "Epoch 89: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 2726.9734 - val_loss: 1839.6282\n",
      "Epoch 90/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3845.6516\n",
      "Epoch 90: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 2939.4287 - val_loss: 2535.4045\n",
      "Epoch 91/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1944.7950\n",
      "Epoch 91: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2735.0002 - val_loss: 2408.7896\n",
      "Epoch 92/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1530.7865\n",
      "Epoch 92: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2649.6404 - val_loss: 2070.6187\n",
      "Epoch 93/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3071.8503\n",
      "Epoch 93: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 2624.6748 - val_loss: 2348.8254\n",
      "Epoch 94/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2951.5542\n",
      "Epoch 94: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 2528.9419 - val_loss: 2070.1079\n",
      "Epoch 95/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3059.8306\n",
      "Epoch 95: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2523.2241 - val_loss: 1921.7146\n",
      "Epoch 96/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3311.5034\n",
      "Epoch 96: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 2501.8306 - val_loss: 2206.9468\n",
      "Epoch 97/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2868.9702\n",
      "Epoch 97: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 2466.6606 - val_loss: 1899.9467\n",
      "Epoch 98/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1269.7891\n",
      "Epoch 98: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 2449.4136 - val_loss: 1646.2142\n",
      "Epoch 99/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1041.2927\n",
      "Epoch 99: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 2297.7607 - val_loss: 2324.6440\n",
      "Epoch 100/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1790.2640\n",
      "Epoch 100: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2425.2175 - val_loss: 1945.1797\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1945.1797\n",
      "Epoch 1/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 792921.7500\n",
      "Epoch 1: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 1s 127ms/step - loss: 761714.2500 - val_loss: 351478.6250\n",
      "Epoch 2/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 720009.3125\n",
      "Epoch 2: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 537437.3125 - val_loss: 150336.6094\n",
      "Epoch 3/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 146333.2969\n",
      "Epoch 3: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 309428.0312 - val_loss: 269678.7188\n",
      "Epoch 4/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 257574.3594\n",
      "Epoch 4: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 182482.0469 - val_loss: 92286.9219\n",
      "Epoch 5/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 299983.2188\n",
      "Epoch 5: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 269779.3125 - val_loss: 139037.3281\n",
      "Epoch 6/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 287902.3125\n",
      "Epoch 6: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 317092.5312 - val_loss: 79123.5391\n",
      "Epoch 7/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 257639.6250\n",
      "Epoch 7: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 206376.4375 - val_loss: 115096.5000\n",
      "Epoch 8/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 143493.3594\n",
      "Epoch 8: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 158706.0000 - val_loss: 238259.8750\n",
      "Epoch 9/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 180231.6250\n",
      "Epoch 9: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 180224.7344 - val_loss: 60517.0117\n",
      "Epoch 10/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 153763.4219\n",
      "Epoch 10: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 115503.6094 - val_loss: 37596.7500\n",
      "Epoch 11/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 142154.7500\n",
      "Epoch 11: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 143035.3594 - val_loss: 32662.2559\n",
      "Epoch 12/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 121218.3281\n",
      "Epoch 12: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 124641.1328 - val_loss: 35368.4297\n",
      "Epoch 13/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 98894.4453\n",
      "Epoch 13: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 93890.3906 - val_loss: 91296.1875\n",
      "Epoch 14/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 107556.0078\n",
      "Epoch 14: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 112332.9062 - val_loss: 53965.2695\n",
      "Epoch 15/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 81152.5391\n",
      "Epoch 15: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 86437.7812 - val_loss: 12968.9189\n",
      "Epoch 16/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 112700.9141\n",
      "Epoch 16: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 80800.2188 - val_loss: 12559.8584\n",
      "Epoch 17/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 82144.2656\n",
      "Epoch 17: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 78567.2812 - val_loss: 8950.3525\n",
      "Epoch 18/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 51763.6914\n",
      "Epoch 18: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 56839.7188 - val_loss: 26927.1191\n",
      "Epoch 19/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 70064.7422\n",
      "Epoch 19: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 63402.0820 - val_loss: 17744.2227\n",
      "Epoch 20/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 52380.1875\n",
      "Epoch 20: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 50160.3281 - val_loss: 7298.0400\n",
      "Epoch 21/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 47538.3438\n",
      "Epoch 21: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 46502.8320 - val_loss: 9288.4307\n",
      "Epoch 22/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 24999.3340\n",
      "Epoch 22: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 38367.5391 - val_loss: 8129.1499\n",
      "Epoch 23/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 40578.2852\n",
      "Epoch 23: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 35971.5625 - val_loss: 11629.8857\n",
      "Epoch 24/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 36390.7227\n",
      "Epoch 24: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 32068.2422 - val_loss: 11160.4453\n",
      "Epoch 25/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 29888.5996\n",
      "Epoch 25: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 28283.1621 - val_loss: 14295.5332\n",
      "Epoch 26/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 29090.9805\n",
      "Epoch 26: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 27331.1465 - val_loss: 9760.2607\n",
      "Epoch 27/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 22451.1035\n",
      "Epoch 27: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 21970.7148 - val_loss: 11259.6885\n",
      "Epoch 28/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 18470.0703\n",
      "Epoch 28: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 22523.0918 - val_loss: 9443.2803\n",
      "Epoch 29/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 16543.2324\n",
      "Epoch 29: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 19778.5273 - val_loss: 10051.8975\n",
      "Epoch 30/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 23701.6309\n",
      "Epoch 30: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 19371.2656 - val_loss: 8171.6694\n",
      "Epoch 31/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 14358.3516\n",
      "Epoch 31: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 16137.2559 - val_loss: 9807.2256\n",
      "Epoch 32/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 20845.5723\n",
      "Epoch 32: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 16173.8418 - val_loss: 7363.0015\n",
      "Epoch 33/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 15077.3623\n",
      "Epoch 33: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 14003.0049 - val_loss: 6006.0371\n",
      "Epoch 34/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 15137.3828\n",
      "Epoch 34: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 14182.5889 - val_loss: 5970.2983\n",
      "Epoch 35/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 13927.7236\n",
      "Epoch 35: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 12886.8154 - val_loss: 8409.4277\n",
      "Epoch 36/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 14459.2598\n",
      "Epoch 36: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 12576.4902 - val_loss: 5752.6113\n",
      "Epoch 37/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 13786.0703\n",
      "Epoch 37: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 11481.1064 - val_loss: 4078.5073\n",
      "Epoch 38/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8971.3369\n",
      "Epoch 38: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 11819.0762 - val_loss: 5672.0000\n",
      "Epoch 39/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 13493.3516\n",
      "Epoch 39: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 10486.6709 - val_loss: 9463.6240\n",
      "Epoch 40/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6698.8584\n",
      "Epoch 40: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 11267.6738 - val_loss: 5072.1196\n",
      "Epoch 41/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 13103.4551\n",
      "Epoch 41: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 10446.4639 - val_loss: 4166.2349\n",
      "Epoch 42/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10434.9893\n",
      "Epoch 42: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 10444.0596 - val_loss: 7280.5405\n",
      "Epoch 43/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 12291.2617\n",
      "Epoch 43: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 9404.3115 - val_loss: 10285.4521\n",
      "Epoch 44/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8799.7480\n",
      "Epoch 44: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 10218.6855 - val_loss: 3736.9773\n",
      "Epoch 45/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9864.5186\n",
      "Epoch 45: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 8948.5400 - val_loss: 3148.0298\n",
      "Epoch 46/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 12684.6406\n",
      "Epoch 46: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 9119.4004 - val_loss: 5254.7690\n",
      "Epoch 47/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9836.3887\n",
      "Epoch 47: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 8371.9561 - val_loss: 4840.7773\n",
      "Epoch 48/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8056.6929\n",
      "Epoch 48: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 8519.0117 - val_loss: 2394.3230\n",
      "Epoch 49/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9151.0957\n",
      "Epoch 49: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 7502.5684 - val_loss: 3444.4187\n",
      "Epoch 50/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6640.6035\n",
      "Epoch 50: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 7307.4824 - val_loss: 3617.9214\n",
      "Epoch 51/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8804.6309\n",
      "Epoch 51: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 6965.3662 - val_loss: 2440.3855\n",
      "Epoch 52/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6578.3193\n",
      "Epoch 52: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 6834.8003 - val_loss: 2681.1694\n",
      "Epoch 53/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6906.3374\n",
      "Epoch 53: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 6536.2476 - val_loss: 3025.6946\n",
      "Epoch 54/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8385.7432\n",
      "Epoch 54: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6352.3423 - val_loss: 2185.6990\n",
      "Epoch 55/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7787.5312\n",
      "Epoch 55: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 6061.3301 - val_loss: 2138.4092\n",
      "Epoch 56/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4823.8975\n",
      "Epoch 56: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 5990.1636 - val_loss: 1752.4971\n",
      "Epoch 57/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6360.5879\n",
      "Epoch 57: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 5988.1553 - val_loss: 1681.6754\n",
      "Epoch 58/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7055.2217\n",
      "Epoch 58: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 5702.6509 - val_loss: 2883.8086\n",
      "Epoch 59/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4800.5742\n",
      "Epoch 59: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 5592.5898 - val_loss: 1946.9857\n",
      "Epoch 60/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3542.7168\n",
      "Epoch 60: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 5614.9458 - val_loss: 1947.8845\n",
      "Epoch 61/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4410.5825\n",
      "Epoch 61: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 5698.7363 - val_loss: 2562.4297\n",
      "Epoch 62/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5895.3335\n",
      "Epoch 62: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 5152.8496 - val_loss: 1313.7928\n",
      "Epoch 63/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3293.1260\n",
      "Epoch 63: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 5062.2593 - val_loss: 2051.6375\n",
      "Epoch 64/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3964.4175\n",
      "Epoch 64: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 4906.2871 - val_loss: 2311.3386\n",
      "Epoch 65/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5442.1172\n",
      "Epoch 65: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 4687.7803 - val_loss: 1451.0067\n",
      "Epoch 66/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3481.1328\n",
      "Epoch 66: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 4610.5229 - val_loss: 2006.6003\n",
      "Epoch 67/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5854.8955\n",
      "Epoch 67: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 4397.2461 - val_loss: 2253.8621\n",
      "Epoch 68/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5273.6807\n",
      "Epoch 68: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 4325.2720 - val_loss: 1571.2117\n",
      "Epoch 69/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4631.8120\n",
      "Epoch 69: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 4118.5864 - val_loss: 1369.9490\n",
      "Epoch 70/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2556.5696\n",
      "Epoch 70: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 4096.7822 - val_loss: 1409.5870\n",
      "Epoch 71/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4676.2095\n",
      "Epoch 71: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 3935.3855 - val_loss: 1311.4182\n",
      "Epoch 72/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2973.2009\n",
      "Epoch 72: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 3836.7944 - val_loss: 1476.8767\n",
      "Epoch 73/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4609.3008\n",
      "Epoch 73: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 3747.9707 - val_loss: 1442.4937\n",
      "Epoch 74/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4414.1626\n",
      "Epoch 74: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 3624.6541 - val_loss: 1553.7914\n",
      "Epoch 75/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4642.0986\n",
      "Epoch 75: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 3648.4028 - val_loss: 1227.9490\n",
      "Epoch 76/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3546.6167\n",
      "Epoch 76: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 3567.1123 - val_loss: 1100.3251\n",
      "Epoch 77/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2256.6208\n",
      "Epoch 77: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 3367.4910 - val_loss: 2030.8635\n",
      "Epoch 78/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4171.0625\n",
      "Epoch 78: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 3453.2314 - val_loss: 1145.3771\n",
      "Epoch 79/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3614.7517\n",
      "Epoch 79: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 3686.0391 - val_loss: 1422.3279\n",
      "Epoch 80/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3433.1145\n",
      "Epoch 80: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 3556.4678 - val_loss: 1787.3939\n",
      "Epoch 81/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1898.5522\n",
      "Epoch 81: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3095.0898 - val_loss: 871.7502\n",
      "Epoch 82/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4134.7427\n",
      "Epoch 82: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 3647.4280 - val_loss: 1523.7983\n",
      "Epoch 83/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1738.2043\n",
      "Epoch 83: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 3170.3887 - val_loss: 1576.5000\n",
      "Epoch 84/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2057.8638\n",
      "Epoch 84: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 2959.7039 - val_loss: 896.7560\n",
      "Epoch 85/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4084.4465\n",
      "Epoch 85: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 3214.7927 - val_loss: 1572.1024\n",
      "Epoch 86/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3542.8535\n",
      "Epoch 86: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 3026.2014 - val_loss: 1182.3815\n",
      "Epoch 87/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3220.2791\n",
      "Epoch 87: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 2655.3035 - val_loss: 710.8908\n",
      "Epoch 88/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1939.9100\n",
      "Epoch 88: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 2945.0745 - val_loss: 1027.6085\n",
      "Epoch 89/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3176.1721\n",
      "Epoch 89: val_loss did not improve from 665.98743\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2600.7603 - val_loss: 983.1620\n",
      "Epoch 90/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3191.4998\n",
      "Epoch 90: val_loss improved from 665.98743 to 661.43311, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 852ms/step - loss: 2534.1934 - val_loss: 661.4331\n",
      "Epoch 91/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1790.5790\n",
      "Epoch 91: val_loss did not improve from 661.43311\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2502.9119 - val_loss: 671.2433\n",
      "Epoch 92/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1412.2361\n",
      "Epoch 92: val_loss did not improve from 661.43311\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2398.8274 - val_loss: 677.5425\n",
      "Epoch 93/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2699.9500\n",
      "Epoch 93: val_loss did not improve from 661.43311\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2352.5320 - val_loss: 770.9944\n",
      "Epoch 94/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2820.0017\n",
      "Epoch 94: val_loss did not improve from 661.43311\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 2319.9224 - val_loss: 728.5089\n",
      "Epoch 95/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2724.8962\n",
      "Epoch 95: val_loss did not improve from 661.43311\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2369.5984 - val_loss: 789.5321\n",
      "Epoch 96/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3020.4583\n",
      "Epoch 96: val_loss improved from 661.43311 to 567.63171, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 712ms/step - loss: 2324.0718 - val_loss: 567.6317\n",
      "Epoch 97/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2572.4231\n",
      "Epoch 97: val_loss improved from 567.63171 to 499.33282, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 836ms/step - loss: 2257.4414 - val_loss: 499.3328\n",
      "Epoch 98/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1227.3014\n",
      "Epoch 98: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 2245.1597 - val_loss: 779.6852\n",
      "Epoch 99/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 976.7655\n",
      "Epoch 99: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2110.2256 - val_loss: 524.9015\n",
      "Epoch 100/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1685.4193\n",
      "Epoch 100: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2215.5945 - val_loss: 602.3799\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 602.3799\n",
      "Epoch 1/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 793017.8125\n",
      "Epoch 1: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 1s 130ms/step - loss: 761769.5625 - val_loss: 646586.6250\n",
      "Epoch 2/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 730745.5000\n",
      "Epoch 2: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 552038.2500 - val_loss: 176044.7344\n",
      "Epoch 3/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 146634.2344\n",
      "Epoch 3: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 338894.3125 - val_loss: 288447.0938\n",
      "Epoch 4/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 306607.7500\n",
      "Epoch 4: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 214625.0312 - val_loss: 243052.7188\n",
      "Epoch 5/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 278190.0000\n",
      "Epoch 5: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 253582.5469 - val_loss: 356200.3438\n",
      "Epoch 6/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 301560.4375\n",
      "Epoch 6: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 339939.3438 - val_loss: 296746.6250\n",
      "Epoch 7/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 317238.3125\n",
      "Epoch 7: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 261471.4219 - val_loss: 155052.8594\n",
      "Epoch 8/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 148079.3281\n",
      "Epoch 8: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 145887.0000 - val_loss: 248058.2812\n",
      "Epoch 9/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 200390.3125\n",
      "Epoch 9: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 211060.3438 - val_loss: 162028.0156\n",
      "Epoch 10/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 171200.6094\n",
      "Epoch 10: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 128200.5859 - val_loss: 142286.3438\n",
      "Epoch 11/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 128124.2188\n",
      "Epoch 11: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 135744.2812 - val_loss: 167466.6406\n",
      "Epoch 12/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 143458.5312\n",
      "Epoch 12: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 153671.6562 - val_loss: 136319.5938\n",
      "Epoch 13/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 124631.9141\n",
      "Epoch 13: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 115430.3438 - val_loss: 115920.2500\n",
      "Epoch 14/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 100255.5312\n",
      "Epoch 14: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 106590.0000 - val_loss: 144217.3281\n",
      "Epoch 15/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 113433.4375\n",
      "Epoch 15: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 112230.3828 - val_loss: 102519.9766\n",
      "Epoch 16/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 107693.2812\n",
      "Epoch 16: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 78564.7344 - val_loss: 97086.9609\n",
      "Epoch 17/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 83030.8516\n",
      "Epoch 17: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 84597.5938 - val_loss: 99348.8828\n",
      "Epoch 18/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 70455.3672\n",
      "Epoch 18: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 80707.0391 - val_loss: 78111.6328\n",
      "Epoch 19/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 79966.4062\n",
      "Epoch 19: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 64150.0977 - val_loss: 81035.8203\n",
      "Epoch 20/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 63974.2344\n",
      "Epoch 20: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 61848.7578 - val_loss: 67913.9453\n",
      "Epoch 21/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 59293.2891\n",
      "Epoch 21: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 51254.4023 - val_loss: 55631.3672\n",
      "Epoch 22/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 24771.6445\n",
      "Epoch 22: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 43359.1523 - val_loss: 52759.2266\n",
      "Epoch 23/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 57579.5430\n",
      "Epoch 23: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 42187.9062 - val_loss: 39914.2070\n",
      "Epoch 24/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 39582.0977\n",
      "Epoch 24: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 33869.8477 - val_loss: 32275.0586\n",
      "Epoch 25/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 33744.4609\n",
      "Epoch 25: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 30924.3711 - val_loss: 27408.2461\n",
      "Epoch 26/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 26820.1895\n",
      "Epoch 26: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 26775.4219 - val_loss: 28736.5879\n",
      "Epoch 27/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 23650.0664\n",
      "Epoch 27: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 26226.1992 - val_loss: 26013.4141\n",
      "Epoch 28/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 21077.3555\n",
      "Epoch 28: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 24246.2188 - val_loss: 18659.1348\n",
      "Epoch 29/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 19426.6934\n",
      "Epoch 29: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 22057.0762 - val_loss: 17812.4121\n",
      "Epoch 30/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 23098.2559\n",
      "Epoch 30: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 19598.2402 - val_loss: 20838.4512\n",
      "Epoch 31/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 16449.9434\n",
      "Epoch 31: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 19141.6914 - val_loss: 19556.8262\n",
      "Epoch 32/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 21908.0820\n",
      "Epoch 32: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 16939.6016 - val_loss: 14526.4785\n",
      "Epoch 33/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 17219.1367\n",
      "Epoch 33: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 16911.3555 - val_loss: 13551.4873\n",
      "Epoch 34/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 16276.3867\n",
      "Epoch 34: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 14915.0039 - val_loss: 17313.0508\n",
      "Epoch 35/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 16642.5020\n",
      "Epoch 35: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 14789.7412 - val_loss: 13493.3379\n",
      "Epoch 36/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 15988.7266\n",
      "Epoch 36: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 13586.7344 - val_loss: 9690.1211\n",
      "Epoch 37/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 14973.7891\n",
      "Epoch 37: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 12729.1738 - val_loss: 9637.4707\n",
      "Epoch 38/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9226.1377\n",
      "Epoch 38: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 12074.1289 - val_loss: 10340.5762\n",
      "Epoch 39/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 14042.4512\n",
      "Epoch 39: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 11399.2881 - val_loss: 6137.9287\n",
      "Epoch 40/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6640.3237\n",
      "Epoch 40: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 11963.0654 - val_loss: 4981.3740\n",
      "Epoch 41/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 14073.8115\n",
      "Epoch 41: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 10578.6816 - val_loss: 9192.5967\n",
      "Epoch 42/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10183.1084\n",
      "Epoch 42: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 11067.5381 - val_loss: 6437.6899\n",
      "Epoch 43/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 11390.6260\n",
      "Epoch 43: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 9002.9521 - val_loss: 4510.4053\n",
      "Epoch 44/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8876.2109\n",
      "Epoch 44: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 11103.0576 - val_loss: 4412.8950\n",
      "Epoch 45/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10067.9561\n",
      "Epoch 45: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 9001.5410 - val_loss: 9696.2744\n",
      "Epoch 46/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 13590.2959\n",
      "Epoch 46: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 10083.3076 - val_loss: 5416.1426\n",
      "Epoch 47/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9954.7080\n",
      "Epoch 47: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 8307.4492 - val_loss: 3924.1257\n",
      "Epoch 48/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9490.3760\n",
      "Epoch 48: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 9027.9756 - val_loss: 4663.5645\n",
      "Epoch 49/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9110.1250\n",
      "Epoch 49: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 7695.4897 - val_loss: 5135.1187\n",
      "Epoch 50/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6925.8291\n",
      "Epoch 50: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 7416.5068 - val_loss: 4162.9019\n",
      "Epoch 51/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8706.1816\n",
      "Epoch 51: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 7059.2056 - val_loss: 3765.0474\n",
      "Epoch 52/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6231.2832\n",
      "Epoch 52: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 6847.5000 - val_loss: 3956.6460\n",
      "Epoch 53/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7132.8203\n",
      "Epoch 53: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 6677.6797 - val_loss: 4196.0044\n",
      "Epoch 54/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8410.5146\n",
      "Epoch 54: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 6505.5024 - val_loss: 3691.7505\n",
      "Epoch 55/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7896.0723\n",
      "Epoch 55: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 6351.5991 - val_loss: 3576.9019\n",
      "Epoch 56/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4758.0449\n",
      "Epoch 56: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 6202.9746 - val_loss: 4793.0576\n",
      "Epoch 57/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6778.9707\n",
      "Epoch 57: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6418.6890 - val_loss: 4447.4722\n",
      "Epoch 58/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7302.5923\n",
      "Epoch 58: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5994.4658 - val_loss: 2627.1062\n",
      "Epoch 59/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5099.3486\n",
      "Epoch 59: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 6053.7388 - val_loss: 3504.2935\n",
      "Epoch 60/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3704.0601\n",
      "Epoch 60: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 5992.2539 - val_loss: 4023.5200\n",
      "Epoch 61/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4667.3862\n",
      "Epoch 61: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 5928.4683 - val_loss: 2438.4453\n",
      "Epoch 62/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6396.4697\n",
      "Epoch 62: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 5321.9634 - val_loss: 3763.0281\n",
      "Epoch 63/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3318.2190\n",
      "Epoch 63: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 5206.6450 - val_loss: 3313.3882\n",
      "Epoch 64/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3857.4280\n",
      "Epoch 64: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4932.2437 - val_loss: 2263.6113\n",
      "Epoch 65/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5847.6426\n",
      "Epoch 65: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 4851.5210 - val_loss: 2714.5513\n",
      "Epoch 66/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3525.5420\n",
      "Epoch 66: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 4668.1860 - val_loss: 2711.9072\n",
      "Epoch 67/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6146.2769\n",
      "Epoch 67: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4615.8462 - val_loss: 1805.0830\n",
      "Epoch 68/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5561.8013\n",
      "Epoch 68: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 4532.2344 - val_loss: 1894.2107\n",
      "Epoch 69/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4693.9199\n",
      "Epoch 69: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4268.4658 - val_loss: 2863.6812\n",
      "Epoch 70/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2785.4390\n",
      "Epoch 70: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 4369.8413 - val_loss: 2354.7446\n",
      "Epoch 71/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5020.1567\n",
      "Epoch 71: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 4121.9868 - val_loss: 1878.5623\n",
      "Epoch 72/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2929.2253\n",
      "Epoch 72: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 3997.7771 - val_loss: 1860.2219\n",
      "Epoch 73/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4923.3867\n",
      "Epoch 73: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 3956.8987 - val_loss: 1627.2656\n",
      "Epoch 74/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4832.6069\n",
      "Epoch 74: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 3791.1230 - val_loss: 1348.8152\n",
      "Epoch 75/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4885.0708\n",
      "Epoch 75: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 3822.8093 - val_loss: 1765.2152\n",
      "Epoch 76/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3931.9980\n",
      "Epoch 76: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 3783.4109 - val_loss: 1838.5115\n",
      "Epoch 77/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2405.9331\n",
      "Epoch 77: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 3544.8813 - val_loss: 1196.2308\n",
      "Epoch 78/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4434.4575\n",
      "Epoch 78: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 3650.1641 - val_loss: 1560.1215\n",
      "Epoch 79/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3894.6438\n",
      "Epoch 79: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 3876.0195 - val_loss: 1220.5527\n",
      "Epoch 80/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3598.0447\n",
      "Epoch 80: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 3692.6982 - val_loss: 1297.6295\n",
      "Epoch 81/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2069.2236\n",
      "Epoch 81: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 3296.0366 - val_loss: 2887.2913\n",
      "Epoch 82/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4398.9321\n",
      "Epoch 82: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 3815.5239 - val_loss: 1345.2616\n",
      "Epoch 83/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1534.3667\n",
      "Epoch 83: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 3230.0928 - val_loss: 1203.8351\n",
      "Epoch 84/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2134.8994\n",
      "Epoch 84: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 3181.7981 - val_loss: 2025.3132\n",
      "Epoch 85/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4281.3838\n",
      "Epoch 85: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 3403.9136 - val_loss: 988.0799\n",
      "Epoch 86/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3616.9302\n",
      "Epoch 86: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 3053.3936 - val_loss: 1067.5504\n",
      "Epoch 87/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3476.8840\n",
      "Epoch 87: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 2901.9785 - val_loss: 1753.3822\n",
      "Epoch 88/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1906.3893\n",
      "Epoch 88: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 3100.2180 - val_loss: 1157.7511\n",
      "Epoch 89/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3266.8650\n",
      "Epoch 89: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2719.3416 - val_loss: 1131.1390\n",
      "Epoch 90/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3782.0562\n",
      "Epoch 90: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 2853.9922 - val_loss: 1246.5184\n",
      "Epoch 91/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1799.8796\n",
      "Epoch 91: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2688.2651 - val_loss: 1034.6517\n",
      "Epoch 92/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1378.5598\n",
      "Epoch 92: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 2601.3096 - val_loss: 833.7535\n",
      "Epoch 93/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3099.0103\n",
      "Epoch 93: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2578.0427 - val_loss: 915.2660\n",
      "Epoch 94/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2904.2117\n",
      "Epoch 94: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2497.2695 - val_loss: 901.4292\n",
      "Epoch 95/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3062.5200\n",
      "Epoch 95: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2521.5266 - val_loss: 870.9247\n",
      "Epoch 96/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3283.8042\n",
      "Epoch 96: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2435.4565 - val_loss: 916.6996\n",
      "Epoch 97/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2918.9226\n",
      "Epoch 97: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 2400.4087 - val_loss: 763.6047\n",
      "Epoch 98/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1175.9460\n",
      "Epoch 98: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2391.6870 - val_loss: 666.3645\n",
      "Epoch 99/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1008.6526\n",
      "Epoch 99: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2269.6289 - val_loss: 809.7609\n",
      "Epoch 100/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1637.9547\n",
      "Epoch 100: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 2378.0171 - val_loss: 664.2755\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 664.2755\n",
      "Epoch 1/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 793473.1250\n",
      "Epoch 1: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 763382.8750 - val_loss: 1042852.3125\n",
      "Epoch 2/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 740080.9375\n",
      "Epoch 2: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 561044.1875 - val_loss: 373978.1562\n",
      "Epoch 3/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 147558.9844\n",
      "Epoch 3: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 350204.6875 - val_loss: 324878.6875\n",
      "Epoch 4/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 307028.4375\n",
      "Epoch 4: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 214725.2969 - val_loss: 532137.0625\n",
      "Epoch 5/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 286980.3125\n",
      "Epoch 5: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 261860.2500 - val_loss: 697002.6250\n",
      "Epoch 6/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 314408.5000\n",
      "Epoch 6: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 356668.1250 - val_loss: 645694.8125\n",
      "Epoch 7/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 352020.4062\n",
      "Epoch 7: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 297256.4375 - val_loss: 424297.7188\n",
      "Epoch 8/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 177451.2500\n",
      "Epoch 8: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 159283.9531 - val_loss: 270760.3125\n",
      "Epoch 9/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 160144.6094\n",
      "Epoch 9: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 195100.9531 - val_loss: 263032.0938\n",
      "Epoch 10/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 213646.0625\n",
      "Epoch 10: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 167130.2188 - val_loss: 293189.1250\n",
      "Epoch 11/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 111992.0234\n",
      "Epoch 11: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 121224.7656 - val_loss: 388653.4375\n",
      "Epoch 12/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 142192.5625\n",
      "Epoch 12: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 159661.5938 - val_loss: 382649.3750\n",
      "Epoch 13/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 152048.4062\n",
      "Epoch 13: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 144354.9844 - val_loss: 278145.5625\n",
      "Epoch 14/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 115335.2578\n",
      "Epoch 14: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 106237.6953 - val_loss: 204791.2812\n",
      "Epoch 15/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 102971.9688\n",
      "Epoch 15: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 109404.6484 - val_loss: 190519.7656\n",
      "Epoch 16/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 125252.2812\n",
      "Epoch 16: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 99148.3750 - val_loss: 203338.2344\n",
      "Epoch 17/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 80006.8438\n",
      "Epoch 17: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 80088.5938 - val_loss: 246133.6406\n",
      "Epoch 18/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 72721.9922\n",
      "Epoch 18: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 89272.0938 - val_loss: 227722.3594\n",
      "Epoch 19/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 102088.7031\n",
      "Epoch 19: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 77642.5156 - val_loss: 160620.4531\n",
      "Epoch 20/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 56554.2930\n",
      "Epoch 20: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 58508.6680 - val_loss: 128336.3359\n",
      "Epoch 21/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 69994.9531\n",
      "Epoch 21: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 62618.0625 - val_loss: 116722.8047\n",
      "Epoch 22/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 33055.4570\n",
      "Epoch 22: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 49877.3555 - val_loss: 124602.3594\n",
      "Epoch 23/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 60373.9922\n",
      "Epoch 23: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 43962.1445 - val_loss: 123816.6406\n",
      "Epoch 24/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 53907.8125\n",
      "Epoch 24: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 43992.6680 - val_loss: 94937.2734\n",
      "Epoch 25/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 39495.5977\n",
      "Epoch 25: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 34859.3125 - val_loss: 67190.9688\n",
      "Epoch 26/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 30935.0645\n",
      "Epoch 26: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 33478.8398 - val_loss: 57392.2344\n",
      "Epoch 27/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 27273.2012\n",
      "Epoch 27: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 29107.4746 - val_loss: 59717.6250\n",
      "Epoch 28/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 25026.4297\n",
      "Epoch 28: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 26591.8516 - val_loss: 53446.2031\n",
      "Epoch 29/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 21573.5684\n",
      "Epoch 29: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 24459.6699 - val_loss: 41428.2500\n",
      "Epoch 30/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 26007.9746\n",
      "Epoch 30: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 22372.1953 - val_loss: 35631.1797\n",
      "Epoch 31/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 20969.2910\n",
      "Epoch 31: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 21468.8535 - val_loss: 35828.0977\n",
      "Epoch 32/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 23949.6055\n",
      "Epoch 32: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 18863.5449 - val_loss: 36526.8984\n",
      "Epoch 33/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 19671.0176\n",
      "Epoch 33: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 17970.8711 - val_loss: 34030.2266\n",
      "Epoch 34/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 17689.2051\n",
      "Epoch 34: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 16413.9199 - val_loss: 32393.4160\n",
      "Epoch 35/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 16920.0996\n",
      "Epoch 35: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 15425.6455 - val_loss: 31673.9648\n",
      "Epoch 36/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 16672.7344\n",
      "Epoch 36: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 14557.3994 - val_loss: 32604.0703\n",
      "Epoch 37/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 16744.7617\n",
      "Epoch 37: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 13751.8008 - val_loss: 33337.1641\n",
      "Epoch 38/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10428.8428\n",
      "Epoch 38: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 13309.8633 - val_loss: 29584.7715\n",
      "Epoch 39/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 15572.6768\n",
      "Epoch 39: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 12085.6514 - val_loss: 25948.7734\n",
      "Epoch 40/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7594.0884\n",
      "Epoch 40: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 12872.7529 - val_loss: 25119.9746\n",
      "Epoch 41/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 15128.9014\n",
      "Epoch 41: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 11469.9385 - val_loss: 29237.4414\n",
      "Epoch 42/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 11502.1846\n",
      "Epoch 42: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 12043.6338 - val_loss: 24087.6211\n",
      "Epoch 43/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 12669.8438\n",
      "Epoch 43: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 9868.2432 - val_loss: 22630.8750\n",
      "Epoch 44/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9839.2490\n",
      "Epoch 44: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 11989.9961 - val_loss: 21566.9668\n",
      "Epoch 45/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10983.2227\n",
      "Epoch 45: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 9759.3311 - val_loss: 26243.5742\n",
      "Epoch 46/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 14626.6758\n",
      "Epoch 46: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 10784.8271 - val_loss: 21866.2344\n",
      "Epoch 47/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10934.9092\n",
      "Epoch 47: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 9058.4209 - val_loss: 20223.5234\n",
      "Epoch 48/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9906.1465\n",
      "Epoch 48: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 9690.4648 - val_loss: 19929.8867\n",
      "Epoch 49/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 10157.2627\n",
      "Epoch 49: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 8431.5850 - val_loss: 19185.6230\n",
      "Epoch 50/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7854.0396\n",
      "Epoch 50: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 8055.5254 - val_loss: 17364.2539\n",
      "Epoch 51/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9531.3623\n",
      "Epoch 51: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 7679.1245 - val_loss: 15460.7168\n",
      "Epoch 52/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7042.3467\n",
      "Epoch 52: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 7508.9419 - val_loss: 14457.9189\n",
      "Epoch 53/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7674.4468\n",
      "Epoch 53: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 7282.5015 - val_loss: 14232.7773\n",
      "Epoch 54/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9193.7734\n",
      "Epoch 54: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 7134.3765 - val_loss: 13435.6240\n",
      "Epoch 55/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8699.8535\n",
      "Epoch 55: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 6898.6611 - val_loss: 13186.5693\n",
      "Epoch 56/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5533.9502\n",
      "Epoch 56: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6764.3291 - val_loss: 14115.1260\n",
      "Epoch 57/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7013.2729\n",
      "Epoch 57: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 7044.2583 - val_loss: 13691.6992\n",
      "Epoch 58/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8145.4351\n",
      "Epoch 58: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 6591.7603 - val_loss: 11070.6035\n",
      "Epoch 59/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5737.4180\n",
      "Epoch 59: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 6587.5898 - val_loss: 10192.9033\n",
      "Epoch 60/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3908.8762\n",
      "Epoch 60: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 6372.8174 - val_loss: 10096.5332\n",
      "Epoch 61/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5366.3140\n",
      "Epoch 61: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6319.1274 - val_loss: 8376.7256\n",
      "Epoch 62/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6687.6665\n",
      "Epoch 62: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 5717.5391 - val_loss: 8350.2607\n",
      "Epoch 63/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3626.5688\n",
      "Epoch 63: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5539.6055 - val_loss: 7912.8125\n",
      "Epoch 64/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4512.0806\n",
      "Epoch 64: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 5311.4771 - val_loss: 6877.2012\n",
      "Epoch 65/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6021.6040\n",
      "Epoch 65: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 5182.5200 - val_loss: 6280.7759\n",
      "Epoch 66/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3624.7747\n",
      "Epoch 66: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 4988.3105 - val_loss: 5696.8721\n",
      "Epoch 67/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6547.8125\n",
      "Epoch 67: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 4947.8213 - val_loss: 4880.9292\n",
      "Epoch 68/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5931.8472\n",
      "Epoch 68: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 4797.2271 - val_loss: 4600.2549\n",
      "Epoch 69/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5130.8472\n",
      "Epoch 69: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 4648.8149 - val_loss: 4695.0957\n",
      "Epoch 70/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3054.3162\n",
      "Epoch 70: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 4576.9419 - val_loss: 4732.8979\n",
      "Epoch 71/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5328.4482\n",
      "Epoch 71: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 4456.3032 - val_loss: 4142.8154\n",
      "Epoch 72/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3287.0452\n",
      "Epoch 72: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 4364.1382 - val_loss: 3630.4783\n",
      "Epoch 73/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5281.5391\n",
      "Epoch 73: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 4236.0122 - val_loss: 3399.6533\n",
      "Epoch 74/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5010.6641\n",
      "Epoch 74: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 4134.2700 - val_loss: 3173.6621\n",
      "Epoch 75/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5263.6484\n",
      "Epoch 75: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 4232.7734 - val_loss: 2924.6729\n",
      "Epoch 76/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4036.8262\n",
      "Epoch 76: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 3978.9458 - val_loss: 2985.2373\n",
      "Epoch 77/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2717.0439\n",
      "Epoch 77: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 3902.9570 - val_loss: 2594.2310\n",
      "Epoch 78/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4616.1650\n",
      "Epoch 78: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 3891.2141 - val_loss: 2161.0410\n",
      "Epoch 79/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4373.0312\n",
      "Epoch 79: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 4109.0615 - val_loss: 1968.5063\n",
      "Epoch 80/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3987.9141\n",
      "Epoch 80: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3745.0979 - val_loss: 2189.6321\n",
      "Epoch 81/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2100.1555\n",
      "Epoch 81: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 3549.2949 - val_loss: 2429.1584\n",
      "Epoch 82/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3942.0730\n",
      "Epoch 82: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 3672.7878 - val_loss: 1890.7991\n",
      "Epoch 83/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1630.2179\n",
      "Epoch 83: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3259.3591 - val_loss: 2086.2510\n",
      "Epoch 84/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2552.8816\n",
      "Epoch 84: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 3504.3018 - val_loss: 1252.0105\n",
      "Epoch 85/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4154.6440\n",
      "Epoch 85: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 3365.1577 - val_loss: 1071.6023\n",
      "Epoch 86/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3734.2092\n",
      "Epoch 86: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 3081.5991 - val_loss: 1846.5931\n",
      "Epoch 87/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3700.2542\n",
      "Epoch 87: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 3362.7930 - val_loss: 1205.5232\n",
      "Epoch 88/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1781.1165\n",
      "Epoch 88: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 3229.4431 - val_loss: 1829.0104\n",
      "Epoch 89/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3562.4966\n",
      "Epoch 89: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 2963.9458 - val_loss: 2029.7728\n",
      "Epoch 90/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4515.9844\n",
      "Epoch 90: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 3541.1003 - val_loss: 1243.4795\n",
      "Epoch 91/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1896.8899\n",
      "Epoch 91: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2821.6323 - val_loss: 1550.3345\n",
      "Epoch 92/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2095.3701\n",
      "Epoch 92: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 3081.6658 - val_loss: 1217.7047\n",
      "Epoch 93/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3420.7844\n",
      "Epoch 93: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2904.0776 - val_loss: 742.4540\n",
      "Epoch 94/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3122.5134\n",
      "Epoch 94: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2794.1265 - val_loss: 782.1210\n",
      "Epoch 95/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3014.4385\n",
      "Epoch 95: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 2541.7058 - val_loss: 1443.8024\n",
      "Epoch 96/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3754.5449\n",
      "Epoch 96: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2881.5569 - val_loss: 796.4315\n",
      "Epoch 97/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3002.5757\n",
      "Epoch 97: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 2674.9863 - val_loss: 837.8733\n",
      "Epoch 98/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1450.1206\n",
      "Epoch 98: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2661.8806 - val_loss: 1030.2310\n",
      "Epoch 99/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1222.1637\n",
      "Epoch 99: val_loss did not improve from 499.33282\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2515.0962 - val_loss: 537.9743\n",
      "Epoch 100/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2004.4386\n",
      "Epoch 100: val_loss improved from 499.33282 to 401.62131, saving model to C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dmmkm\\Documents\\Projects\\battery_management_systems\\model_checkpoints\\simpleMLP_checkpoints.ckpt\\assets\n",
      "2/2 [==============================] - 1s 794ms/step - loss: 2543.3997 - val_loss: 401.6213\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 401.6213\n"
     ]
    }
   ],
   "source": [
    "cv=KFold(n_splits=10, random_state=random_seed, shuffle=True)\n",
    "model_performances=[]\n",
    "model_weights=[]\n",
    "\n",
    "# create checkpoint\n",
    "checkpoint_path=project_path+'\\model_checkpoints\\simpleMLP_checkpoints.ckpt'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# train model\n",
    "for train_index, val_index in cv.split(x_train):\n",
    "    x_train1, x_val = x_train[train_index], x_train[val_index]\n",
    "    y_train1, y_val = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    model=create_model(learning_rate=0.01, alpha=0.1, nodes=279)\n",
    "    history=model.fit(x_train, y_train, epochs=100, batch_size=64, validation_data=(x_val, y_val), callbacks=[cp_callback])\n",
    "    model_performances.append(model.evaluate(x_val, y_val))\n",
    "    model_weight=model.get_weights()\n",
    "    model_weights.append(model_weight)\n",
    "    model_dict=dict(zip(model_performances, model_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2809.525146484375,\n",
       " 1026.4759521484375,\n",
       " 826.18310546875,\n",
       " 9681.712890625,\n",
       " 2296.507568359375,\n",
       " 1095.0999755859375,\n",
       " 1945.1796875,\n",
       " 602.3798828125,\n",
       " 664.2754516601562,\n",
       " 401.6213073730469]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 2134.8960968017577 Stdev: 2626.9707281597925 Best: 401.6213073730469\n"
     ]
    }
   ],
   "source": [
    "mean_performance=np.mean(np.array(model_performances))\n",
    "std_performance=np.std(np.array(model_performances))\n",
    "best_performance=np.min(np.array(model_performances))\n",
    "\n",
    "print('Mean:', mean_performance, 'Stdev:', std_performance, 'Best:', best_performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'MSE')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(model_performances, bins=30)\n",
    "plt.ylabel('MSE Counts')\n",
    "plt.xlabel('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Best Model from training\n",
    "model = tf.keras.models.load_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Learning Curve\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (None, 279)               78120     \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 279)               78120     \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 279)               78120     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 280       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 234,640\n",
      "Trainable params: 234,640\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# print Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using test set\n",
    "y_pred=model.predict(x_test)\n",
    "result_dict={'actual':y_test,\n",
    "      'predicted':list(y_pred.T[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>residuals</th>\n",
       "      <th>% Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>915.0</td>\n",
       "      <td>854.710388</td>\n",
       "      <td>60.289612</td>\n",
       "      <td>6.589029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>169.0</td>\n",
       "      <td>330.146729</td>\n",
       "      <td>-161.146729</td>\n",
       "      <td>95.353094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>597.0</td>\n",
       "      <td>630.367493</td>\n",
       "      <td>-33.367493</td>\n",
       "      <td>5.589195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>491.0</td>\n",
       "      <td>520.142334</td>\n",
       "      <td>-29.142334</td>\n",
       "      <td>5.935302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>519.0</td>\n",
       "      <td>488.217041</td>\n",
       "      <td>30.782959</td>\n",
       "      <td>5.931206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual   predicted   residuals    % Error\n",
       "0   915.0  854.710388   60.289612   6.589029\n",
       "1   169.0  330.146729 -161.146729  95.353094\n",
       "2   597.0  630.367493  -33.367493   5.589195\n",
       "3   491.0  520.142334  -29.142334   5.935302\n",
       "4   519.0  488.217041   30.782959   5.931206"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df=pd.DataFrame.from_dict(result_dict)\n",
    "result_df['residuals']=result_df['actual']-result_df['predicted']\n",
    "result_df[\"% Error\"] = (abs(result_df['actual'] - result_df['predicted'])/result_df['actual'])*100\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.37356944902651745\n",
      "MSE: 82795.72683771781\n"
     ]
    }
   ],
   "source": [
    "# R^2 score\n",
    "r2=r2_score(result_dict['actual'], result_dict['predicted'])\n",
    "print('R^2:',r2)\n",
    "\n",
    "mse=mean_squared_error(result_dict['actual'], result_dict['predicted'])\n",
    "print('MSE:', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit line\n",
    "x=list(range(int(min(y_test)), int(max(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Actual and Predicted Results\n",
    "plt.scatter(result_dict['actual'], result_dict['predicted'])\n",
    "plt.plot(x, x, color='black')\n",
    "plt.title('Predicted vs Actual '+target, fontsize=14)\n",
    "plt.xlabel('Actual '+target, fontsize=14)\n",
    "plt.ylabel('Predicted '+target, fontsize=14)\n",
    "\n",
    "plt.text(75, 2300, 'R^2 = '+str(round(r2, 3)), fontsize=16)\n",
    "plt.text(1600, 2300, 'MSE = '+str(round(mse, 0)), fontsize=16)\n",
    "plt.ylim(0, 2500)\n",
    "plt.xlim(0, 2500)\n",
    "\n",
    "#plt.ylim(20000, 120000)\n",
    "#plt.xlim(20000, 120000)\n",
    "#plt.text(22000, 113000, 'R^2 = '+str(round(r2, 3)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAGwCAYAAABcnuQpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAst0lEQVR4nO3de1xU1f7/8feAMuIFFPFGgaKZmJKXk/I1zTBJstLM7ykzK7NztItlhseMR5nZqdBuWubR6pvS6WadUut0sSto3jIULRIVOSSUolIK4QUU1u+P83B+TYAiwswsfD0fj/14sNdee+/PYjszb/esGRzGGCMAAABL+Xm7AAAAgDNBmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGqEGQAAYDXCDAAAsFoDbxdQ18rLy7V79241a9ZMDofD2+UAAIBqMMbot99+U1hYmPz8Tn7vpd6Hmd27dys8PNzbZQAAgBrIy8vTueeee9I+9T7MNGvWTNJ/fxlBQUFergYAAFRHUVGRwsPDXa/jJ1Pvw8yJt5aCgoIIMwAAWKY6U0SYAAwAAKxGmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGqEGQAAYDXCDAAAsBphBgAAWI0wAwAArEaYAQAAViPMAAAAq3k1zKxatUrDhg1TWFiYHA6Hli9fXqFPZmamhg8fruDgYDVp0kR9+vRRbm6u54sFAAA+yath5tChQ+rRo4fmz59f6fbs7GwNGDBAUVFRSk1N1Xfffafp06erUaNGHq4UAAD4Kocxxni7COm/fxVz2bJlGjFihKvthhtuUMOGDfXaa6/V+LhFRUUKDg5WYWEhfzUbAABLnM7rdwMP1XTaysvL9dFHH+n+++9XfHy80tPTFRkZqcTERLfA80clJSUqKSlxrRcVFXmgWgCovtzcXBUUFHj8vKGhoYqIiPD4eYG65rNhZt++fSouLtasWbP02GOPafbs2VqxYoVGjhyplJQUXXrppZXul5SUpJkzZ3q4WgContzcXEVFRenIkSMeP3dgYKC2bdtGoEG947Nhpry8XJJ0zTXX6L777pMk9ezZU2vXrtXChQurDDOJiYlKSEhwrRcVFSk8PLzuCwaAaigoKNCRI0c0cuRIhYaGevS8S5cuVUFBAWEG9Y7PhpnQ0FA1aNBAF1xwgVt7165dtXr16ir3czqdcjqddV0eAJyR0NBQhYWFebsMoF7w2e+ZCQgIUJ8+fbR9+3a39h07dqh9+/ZeqgoAAPgar96ZKS4u1s6dO13rOTk52rx5s0JCQhQREaGpU6dq1KhRGjhwoAYNGqQVK1bo3//+t1JTU71XNAAA8CleDTNpaWkaNGiQa/3EXJexY8cqOTlZ1157rRYuXKikpCRNmjRJXbp00XvvvacBAwZ4q2QAAOBjvBpmYmNjdaqvubntttt02223eagiAABgG5+dMwMAAFAdhBkAAGA1wgwAALAaYQYAAFiNMAMAAKxGmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGqEGQAAYDXCDAAAsBphBgAAWI0wAwAArEaYAQAAViPMAAAAqxFmAACA1QgzAADAaoQZAABgNcIMAACwGmEGAABYjTADAACsRpgBAABWI8wAAACrEWYAAIDVCDMAAMBqhBkAAGA1wgwAALAaYQYAAFiNMAMAAKxGmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGpeDTOrVq3SsGHDFBYWJofDoeXLl1fZ94477pDD4dDcuXM9Vh8AAPB9Xg0zhw4dUo8ePTR//vyT9lu2bJnWr1+vsLAwD1UGAABs0cCbJx86dKiGDh160j4///yz7rnnHn366ae66qqrPFQZAACwhVfDzKmUl5fr5ptv1tSpU9WtW7dq7VNSUqKSkhLXelFRUV2VBwAAfIBPTwCePXu2GjRooEmTJlV7n6SkJAUHB7uW8PDwOqwQAAB4m8+GmY0bN+q5555TcnKyHA5HtfdLTExUYWGha8nLy6vDKgEAgLf5bJj5+uuvtW/fPkVERKhBgwZq0KCBdu3apSlTpqhDhw5V7ud0OhUUFOS2AACA+stn58zcfPPNiouLc2uLj4/XzTffrHHjxnmpKgAA4Gu8GmaKi4u1c+dO13pOTo42b96skJAQRUREqGXLlm79GzZsqLZt26pLly6eLhUAAPgor4aZtLQ0DRo0yLWekJAgSRo7dqySk5O9VBUAALCJV8NMbGysjDHV7v/jjz/WXTEAAMBKPjsBGAAAoDoIMwAAwGqEGQAAYDXCDAAAsBphBgAAWI0wAwAArEaYAQAAViPMAAAAqxFmAACA1QgzAADAaoQZAABgNcIMAACwGmEGAABYjTADAACsRpgBAABWI8wAAACrEWYAAIDVCDMAAMBqhBkAAGA1wgwAALAaYQYAAFiNMAMAAKxGmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGqEGQAAYDXCDAAAsBphBgAAWI0wAwAArEaYAQAAViPMAAAAqxFmAACA1bwaZlatWqVhw4YpLCxMDodDy5cvd207duyYpk2bpujoaDVp0kRhYWG65ZZbtHv3bu8VDAAAfI5Xw8yhQ4fUo0cPzZ8/v8K2w4cPa9OmTZo+fbo2bdqkpUuXavv27Ro+fLgXKgUAAL6qgTdPPnToUA0dOrTSbcHBwfr888/d2l544QX17dtXubm5ioiI8ESJAADAx3k1zJyuwsJCORwONW/evMo+JSUlKikpca0XFRV5oDIAAOAt1kwAPnr0qKZNm6bRo0crKCioyn5JSUkKDg52LeHh4R6sEgAAeJoVYebYsWO6/vrrZYzRggULTto3MTFRhYWFriUvL89DVQIAAG/w+beZTgSZXbt26auvvjrpXRlJcjqdcjqdHqoOAAB4m0+HmRNBJisrSykpKWrZsqW3SwIAAD7Gq2GmuLhYO3fudK3n5ORo8+bNCgkJUbt27fTnP/9ZmzZt0ocffqiysjLl5+dLkkJCQhQQEOCtsgEAgA/xaphJS0vToEGDXOsJCQmSpLFjx+qRRx7RBx98IEnq2bOn234pKSmKjY31VJkAAMCHeTXMxMbGyhhT5faTbQMAAJAs+TQTAABAVQgzAADAaoQZAABgNcIMAACwGmEGAABYjTADAACsRpgBAABWI8wAAACrEWYAAIDVCDMAAMBqhBkAAGA1wgwAALAaYQYAAFiNMAMAAKxGmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGqEGQAAYDXCDAAAsBphBgAAWI0wAwAArEaYAQAAViPMAAAAqxFmAACA1QgzAADAaoQZAABgNcIMAACwGmEGAABYjTADAACsRpgBAABWI8wAAACrEWYAAIDVvBpmVq1apWHDhiksLEwOh0PLly93226M0cMPP6x27dopMDBQcXFxysrK8k6xAADAJ3k1zBw6dEg9evTQ/PnzK93+5JNP6vnnn9fChQv1zTffqEmTJoqPj9fRo0c9XCkAAPBVDbx58qFDh2ro0KGVbjPGaO7cuXrooYd0zTXXSJL++c9/qk2bNlq+fLluuOEGT5YKAAB8lM/OmcnJyVF+fr7i4uJcbcHBwYqJidG6deuq3K+kpERFRUVuCwAAqL98Nszk5+dLktq0aePW3qZNG9e2yiQlJSk4ONi1hIeH12mdAADAu3w2zNRUYmKiCgsLXUteXp63SwIAAHXIZ8NM27ZtJUl79+51a9+7d69rW2WcTqeCgoLcFgAAUH/5bJiJjIxU27Zt9eWXX7raioqK9M0336hfv35erAwAAPgSr36aqbi4WDt37nSt5+TkaPPmzQoJCVFERIQmT56sxx57TJ07d1ZkZKSmT5+usLAwjRgxwntFAwAAn+LVMJOWlqZBgwa51hMSEiRJY8eOVXJysu6//34dOnRIEyZM0MGDBzVgwACtWLFCjRo18lbJAADAx3g1zMTGxsoYU+V2h8OhRx99VI8++qgHqwIAADbx2TkzAAAA1UGYAQAAViPMAAAAqxFmAACA1QgzAADAaoQZAABgNcIMAACwGmEGAABYrUZhJi8vTz/99JNrfcOGDZo8ebJeeumlWisMAACgOmoUZm688UalpKRIkvLz83X55Zdrw4YNevDBB/m2XgAA4FE1CjMZGRnq27evJOmdd95R9+7dtXbtWr3xxhtKTk6uzfoAAABOqkZh5tixY3I6nZKkL774QsOHD5ckRUVFac+ePbVXHQAAwCnUKMx069ZNCxcu1Ndff63PP/9cV1xxhSRp9+7datmyZa0WCAAAcDI1CjOzZ8/Wiy++qNjYWI0ePVo9evSQJH3wwQeut58AAAA8oUFNdoqNjVVBQYGKiorUokULV/uECRPUpEmTWisOAADgVGp0Z+ayyy7Tb7/95hZkJCkkJESjRo2qlcIAAACqo0ZhJjU1VaWlpRXajx49qq+//vqMiwIAAKiu03qb6bvvvnP9vHXrVuXn57vWy8rKtGLFCp1zzjm1Vx0AAMApnFaY6dmzpxwOhxwOhy677LIK2wMDAzVv3rxaKw4AAOBUTivM5OTkyBijjh07asOGDWrVqpVrW0BAgFq3bi1/f/9aLxIAAKAqpxVm2rdvL0kqLy+vk2IAAABOV40+mi1JWVlZSklJ0b59+yqEm4cffviMCwMAAKiOGoWZl19+WXfeeadCQ0PVtm1bORwO1zaHw0GYAQAAHlOjMPPYY4/p8ccf17Rp02q7HgAAgNNSo++ZOXDggK677rrargUAAOC01SjMXHfddfrss89quxYAAIDTVqO3mc477zxNnz5d69evV3R0tBo2bOi2fdKkSbVSHAAAwKnUKMy89NJLatq0qVauXKmVK1e6bXM4HIQZAADgMTUKMzk5ObVdBwAAQI3UaM4MAACAr6jRnZnbbrvtpNsXLVpUo2IAAABOV43CzIEDB9zWjx07poyMDB08eLDSP0AJAABQV2oUZpYtW1ahrby8XHfeeac6dep0xkUBAABUV63NmfHz81NCQoLmzJlTW4dUWVmZpk+frsjISAUGBqpTp076+9//LmNMrZ0DAADYrcZ/aLIy2dnZOn78eK0db/bs2VqwYIFeffVVdevWTWlpaRo3bpyCg4P5+DcAAJBUwzCTkJDgtm6M0Z49e/TRRx9p7NixtVKYJK1du1bXXHONrrrqKklShw4d9NZbb2nDhg21dg4AAGC3GoWZ9PR0t3U/Pz+1atVKzzzzzCk/6XQ6Lr74Yr300kvasWOHzj//fG3ZskWrV6/Ws88+W+U+JSUlKikpca0XFRXVWj0AAMD31CjMpKSk1HYdlXrggQdUVFSkqKgo+fv7q6ysTI8//rjGjBlT5T5JSUmaOXOmR+oDAADed0YTgPfv36/Vq1dr9erV2r9/f23V5PLOO+/ojTfe0JtvvqlNmzbp1Vdf1dNPP61XX321yn0SExNVWFjoWvLy8mq9LgAA4DtqdGfm0KFDuueee/TPf/5T5eXlkiR/f3/dcsstmjdvnho3blwrxU2dOlUPPPCAbrjhBklSdHS0du3apaSkpCrn5jidTjmdzlo5PwAA8H01ujOTkJCglStX6t///rcOHjyogwcP6v3339fKlSs1ZcqUWivu8OHD8vNzL9Hf398VoAAAAGp0Z+a9997Tu+++q9jYWFfblVdeqcDAQF1//fVasGBBrRQ3bNgwPf7444qIiFC3bt2Unp6uZ599tlYnGQMAALvVKMwcPnxYbdq0qdDeunVrHT58+IyLOmHevHmaPn267rrrLu3bt09hYWG6/fbb9fDDD9faOQAAgN1q9DZTv379NGPGDB09etTVduTIEc2cOVP9+vWrteKaNWumuXPnateuXTpy5Iiys7P12GOPKSAgoNbOAQAA7FajOzNz587VFVdcoXPPPVc9evSQJG3ZskVOp1OfffZZrRYIAABwMjUKM9HR0crKytIbb7yhbdu2SZJGjx6tMWPGKDAwsFYLBAAAOJkahZmkpCS1adNG48ePd2tftGiR9u/fr2nTptVKcQAAAKdSozkzL774oqKioiq0d+vWTQsXLjzjogAAAKqrRmEmPz9f7dq1q9DeqlUr7dmz54yLAgAAqK4ahZnw8HCtWbOmQvuaNWsUFhZ2xkUBAABUV43mzIwfP16TJ0/WsWPHdNlll0mSvvzyS91///21+g3AAAAAp1KjMDN16lT98ssvuuuuu1RaWipJatSokaZNm6bExMRaLRAAAOBkahRmHA6HZs+erenTpyszM1OBgYHq3Lkzf+ARAAB4XI3CzAlNmzZVnz59aqsWAACA01ajCcAAAAC+gjADAACsRpgBAABWI8wAAACrEWYAAIDVCDMAAMBqhBkAAGA1wgwAALAaYQYAAFiNMAMAAKxGmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGqEGQAAYDXCDAAAsBphBgAAWI0wAwAArEaYAQAAViPMAAAAqxFmAACA1QgzAADAaoQZAABgNZ8PMz///LNuuukmtWzZUoGBgYqOjlZaWpq3ywIAAD6igbcLOJkDBw6of//+GjRokD755BO1atVKWVlZatGihbdLAwAAPsKnw8zs2bMVHh6uxYsXu9oiIyO9WBEAAPA1Pv020wcffKCLLrpI1113nVq3bq1evXrp5ZdfPuk+JSUlKioqclsAAED95dNh5j//+Y8WLFigzp0769NPP9Wdd96pSZMm6dVXX61yn6SkJAUHB7uW8PBwD1YMAAA8zafDTHl5uXr37q0nnnhCvXr10oQJEzR+/HgtXLiwyn0SExNVWFjoWvLy8jxYMQAA8DSfDjPt2rXTBRdc4NbWtWtX5ebmVrmP0+lUUFCQ2wIAAOovnw4z/fv31/bt293aduzYofbt23upIgAA4Gt8Oszcd999Wr9+vZ544gnt3LlTb775pl566SVNnDjR26UBAAAf4dNhpk+fPlq2bJneeustde/eXX//+981d+5cjRkzxtulAQAAH+HT3zMjSVdffbWuvvpqb5cBAAB8lE/fmQEAADgVwgwAALAaYQYAAFiNMAMAAKxGmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGqEGQAAYDXCDAAAsBphBgAAWI0wAwAArEaYAQAAViPMAAAAqxFmAACA1QgzAADAaoQZAABgNcIMAACwGmEGAABYjTADAACsRpgBAABWI8wAAACrEWYAAIDVCDMAAMBqhBkAAGA1wgwAALAaYQYAAFiNMAMAAKxGmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGqEGQAAYDWrwsysWbPkcDg0efJkb5cCAAB8hDVh5ttvv9WLL76oCy+80NulAAAAH2JFmCkuLtaYMWP08ssvq0WLFt4uBwAA+BArwszEiRN11VVXKS4u7pR9S0pKVFRU5LYAAID6q4G3CziVJUuWaNOmTfr222+r1T8pKUkzZ86s46oAAICv8Ok7M3l5ebr33nv1xhtvqFGjRtXaJzExUYWFha4lLy+vjqsEAADe5NN3ZjZu3Kh9+/apd+/erraysjKtWrVKL7zwgkpKSuTv7++2j9PplNPp9HSpAADAS3w6zAwePFjff/+9W9u4ceMUFRWladOmVQgyAADg7OPTYaZZs2bq3r27W1uTJk3UsmXLCu0AAODs5NNzZgAAAE7Fp+/MVCY1NdXbJQAAAB/CnRkAAGA1wgwAALAaYQYAAFiNMAMAAKxGmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGqEGQAAYDXCDAAAsBphBgAAWI0wAwAArEaYAQAAViPMAAAAqxFmAACA1QgzAADAaoQZAABgNcIMAACwGmEGAABYjTADAACsRpgBAABWI8wAAACrEWYAAIDVCDMAAMBqhBkAAGA1wgwAALAaYQYAAFiNMAMAAKxGmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGqEGQAAYDWfDzNJSUnq06ePmjVrptatW2vEiBHavn27t8sCAAA+wufDzMqVKzVx4kStX79en3/+uY4dO6YhQ4bo0KFD3i4NAAD4gAbeLuBUVqxY4baenJys1q1ba+PGjRo4cKCXqgIAAL7C58PMHxUWFkqSQkJCKt1eUlKikpIS13pRUZFH6gJQc7m5uSooKPD4eUNDQxUREeHx83pTZmamx89ZUlIip9Pp8fOejdf3bGVVmCkvL9fkyZPVv39/de/evdI+SUlJmjlzpocrA1BTubm5ioqK0pEjRzx+7sDAQG3btu2seMErLi6Ww+HQTTfd5PFzOxwOGWM8ft6z6fqe7awKMxMnTlRGRoZWr15dZZ/ExEQlJCS41ouKihQeHu6J8gDUQEFBgY4cOaKRI0cqNDTUo+ddunSpCgoKzooXu6NHj8oY4/Hfc1ZWllJSUri+qFPWhJm7775bH374oVatWqVzzz23yn5Op9MrtzMBnJnQ0FCFhYV5u4x6z9O/5xNvH3J9UZd8PswYY3TPPfdo2bJlSk1NVWRkpLdLAgAAPsTnw8zEiRP15ptv6v3331ezZs2Un58vSQoODlZgYKCXqwMAAN7m898zs2DBAhUWFio2Nlbt2rVzLW+//ba3SwMAAD7A5+/MeGMGPAAAsIfP35kBAAA4GcIMAACwGmEGAABYjTADAACsRpgBAABWI8wAAACrEWYAAIDVCDMAAMBqhBkAAGA1wgwAALAaYQYAAFiNMAMAAKxGmAEAAFYjzAAAAKs18HYBtsvNzVVBQYHHzxsaGqqIiAiPnxeobzIzM+v1+XD2OJtfjwgzZyA3N1dRUVE6cuSIx88dGBiobdu2ef0fEGCr4uJiORwO3XTTTd4uBThjZ/vrEWHmDBQUFOjIkSMaOXKkQkNDPXrepUuXqqCggDAD1NDRo0dljPH44zcrK0spKSkeOx/ODmf76xFhphaEhoYqLCzM22UAqAFPP3698TYAzh5n6+sRE4ABAIDVCDMAAMBqhBkAAGA1wgwAALAaYQYAAFiNMAMAAKxGmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGqEGQAAYDXCDAAAsBphBgAAWI0wAwAArEaYAQAAVrMizMyfP18dOnRQo0aNFBMTow0bNni7JAAA4CN8Psy8/fbbSkhI0IwZM7Rp0yb16NFD8fHx2rdvn7dLAwAAPsDnw8yzzz6r8ePHa9y4cbrgggu0cOFCNW7cWIsWLfJ2aQAAwAc08HYBJ1NaWqqNGzcqMTHR1ebn56e4uDitW7eu0n1KSkpUUlLiWi8sLJQkFRUV1Xp9xcXFkqQ9e/aotLS01o9flV9++UWStHHjRlcNnuLn56fy8nKPnpPz1u/zbt++XZLnH0f79+/nvPX4vGfb86S3Hkcnfs/FxcW1/jp74njGmFN3Nj7s559/NpLM2rVr3dqnTp1q+vbtW+k+M2bMMJJYWFhYWFhY6sGSl5d3yrzg03dmaiIxMVEJCQmu9fLycv36669q2bKlHA6HW9+ioiKFh4crLy9PQUFBni7VKxjz2TFm6ewcN2NmzPXZ2TZuY4x+++03hYWFnbKvT4eZ0NBQ+fv7a+/evW7te/fuVdu2bSvdx+l0yul0urU1b978pOcJCgo6K/5h/B5jPnucjeNmzGeHs3HM0tk17uDg4Gr18+kJwAEBAfrTn/6kL7/80tVWXl6uL7/8Uv369fNiZQAAwFf49J0ZSUpISNDYsWN10UUXqW/fvpo7d64OHTqkcePGebs0AADgA3w+zIwaNUr79+/Xww8/rPz8fPXs2VMrVqxQmzZtzvjYTqdTM2bMqPC2VH3GmM8eZ+O4GfPZ4Wwcs3T2jrs6HMZU5zNPAAAAvsmn58wAAACcCmEGAABYjTADAACsRpgBAABWq5dh5vHHH9fFF1+sxo0bV/qFeVu2bNHo0aMVHh6uwMBAde3aVc8995xbn9TUVDkcjgpLfn6+W7/58+erQ4cOatSokWJiYrRhw4a6HFqVTjVmScrNzdVVV12lxo0bq3Xr1po6daqOHz/u1ic1NVW9e/eW0+nUeeedp+Tk5ArH8ZUx/1FV18zhcOjbb7+VJP3444+Vbl+/fr3bsf71r38pKipKjRo1UnR0tD7++GNvDKlaOnToUGE8s2bNcuvz3Xff6ZJLLlGjRo0UHh6uJ598ssJxbBnzjz/+qL/85S+KjIxUYGCgOnXqpBkzZrj9PZr6eJ2r4quPx9OVlJSkPn36qFmzZmrdurVGjBjh+ntDJ8TGxla4pnfccYdbn+o8z/mKRx55pMJ4oqKiXNuPHj2qiRMnqmXLlmratKn+93//t8KXyNo03jp15n9Byfc8/PDD5tlnnzUJCQkmODi4wvZXXnnFTJo0yaSmpprs7Gzz2muvmcDAQDNv3jxXn5SUFCPJbN++3ezZs8e1lJWVufosWbLEBAQEmEWLFpkffvjBjB8/3jRv3tzs3bvXE8N0c6oxHz9+3HTv3t3ExcWZ9PR08/HHH5vQ0FCTmJjo6vOf//zHNG7c2CQkJJitW7eaefPmGX9/f7NixQpXH18a8x+VlJS4Xas9e/aYv/71ryYyMtKUl5cbY4zJyckxkswXX3zh1q+0tNR1nDVr1hh/f3/z5JNPmq1bt5qHHnrINGzY0Hz//ffeGtpJtW/f3jz66KNu4ykuLnZtLywsNG3atDFjxowxGRkZ5q233jKBgYHmxRdfdPWxacyffPKJufXWW82nn35qsrOzzfvvv29at25tpkyZ4upTH69zZXz58Xi64uPjzeLFi01GRobZvHmzufLKK01ERITbv+VLL73UjB8/3u2aFhYWurZX53nOl8yYMcN069bNbTz79+93bb/jjjtMeHi4+fLLL01aWpr5n//5H3PxxRe7tts23rpUL8PMCYsXL670hb0yd911lxk0aJBr/USYOXDgQJX79O3b10ycONG1XlZWZsLCwkxSUlJNSz5jVY35448/Nn5+fiY/P9/VtmDBAhMUFGRKSkqMMcbcf//9plu3bm77jRo1ysTHx7vWfXHMVSktLTWtWrUyjz76qKvtxItcenp6lftdf/315qqrrnJri4mJMbfffntdlXpG2rdvb+bMmVPl9n/84x+mRYsWrutsjDHTpk0zXbp0ca3bNuY/evLJJ01kZKRrvT5e58rY9Hg8Xfv27TOSzMqVK11tl156qbn33nur3Kc6z3O+ZMaMGaZHjx6Vbjt48KBp2LCh+de//uVqy8zMNJLMunXrjDH2jbcu1cu3mWqisLBQISEhFdp79uypdu3a6fLLL9eaNWtc7aWlpdq4caPi4uJcbX5+foqLi9O6des8UvPpWLdunaKjo92+bDA+Pl5FRUX64YcfXH1+P54TfU6Mx7Yxf/DBB/rll18q/bbo4cOHq3Xr1howYIA++OADt22n+j34olmzZqlly5bq1auXnnrqKbfbzOvWrdPAgQMVEBDgaouPj9f27dt14MABVx/bxvx7VT1+69t1/j3bHo+nq7CwUJIqXNc33nhDoaGh6t69uxITE3X48GHXtuo8z/marKwshYWFqWPHjhozZoxyc3MlSRs3btSxY8fcrm9UVJQiIiJc19fG8dYVn/8GYE9Yu3at3n77bX300Ueutnbt2mnhwoW66KKLVFJSov/7v/9TbGysvvnmG/Xu3VsFBQUqKyur8E3Ebdq00bZt2zw9hFPKz8+vtNYT207Wp6ioSEeOHNGBAwesGvMrr7yi+Ph4nXvuua62pk2b6plnnlH//v3l5+en9957TyNGjNDy5cs1fPhwSVX/Hv44X8pXTJo0Sb1791ZISIjWrl2rxMRE7dmzR88++6yk/44nMjLSbZ/fX/sWLVpYN+bf27lzp+bNm6enn37a1VYfr/Mf2fYcdDrKy8s1efJk9e/fX927d3e133jjjWrfvr3CwsL03Xffadq0adq+fbuWLl0qqXrPc74kJiZGycnJ6tKli/bs2aOZM2fqkksuUUZGhvLz8xUQEFBhDuTv/43aNt66ZE2YeeCBBzR79uyT9snMzHSbPFUdGRkZuuaaazRjxgwNGTLE1d6lSxd16dLFtX7xxRcrOztbc+bM0WuvvXZ6xddQXY3ZNjX5Pfz000/69NNP9c4777j1Cw0NVUJCgmu9T58+2r17t5566inXi5wvOJ0x/348F154oQICAnT77bcrKSnJqq89r8l1/vnnn3XFFVfouuuu0/jx413ttlxnVG7ixInKyMjQ6tWr3donTJjg+jk6Olrt2rXT4MGDlZ2drU6dOnm6zDM2dOhQ188XXnihYmJi1L59e73zzjsKDAz0YmX2sSbMTJkyRbfeeutJ+3Ts2PG0jrl161YNHjxYEyZM0EMPPXTK/n379nU9uEJDQ+Xv719hZvnevXvVtm3b06qjKrU55rZt21b4lMOJ2k/U27Zt20rHExQUpMDAQPn7+9f5mCtTk9/D4sWL1bJly2q9cMXExOjzzz93rVf1e6jLMf7RmVz7mJgYHT9+XD/++KO6dOlS5XikU197Xx7z7t27NWjQIF188cV66aWXTnl8X7zOZ8ITz0HecPfdd+vDDz/UqlWr3O6qViYmJkbSf+/OderUqVrPc76sefPmOv/887Vz505dfvnlKi0t1cGDB93uzvz++to+3lrl7Uk7delkE4AzMjJM69atzdSpU6t9vLi4OHPttde61vv27Wvuvvtu13pZWZk555xzfHoC8O8/5fDiiy+aoKAgc/ToUWPMfycAd+/e3W2/0aNHV5gA7Gtj/qPy8nITGRnp9umWk/nrX/9qevXq5Vq//vrrzdVXX+3Wp1+/ftZMDH399deNn5+f+fXXX40x/38C8O8/yZOYmFhhArBNY/7pp59M586dzQ033GCOHz9erX3q23U2xo7HY3WVl5ebiRMnmrCwMLNjx45q7bN69WojyWzZssUYU73nOV/222+/mRYtWpjnnnvONQH43XffdW3ftm1bpROAbR1vbaqXYWbXrl0mPT3dzJw50zRt2tSkp6eb9PR089tvvxljjPn+++9Nq1atzE033eT2kbh9+/a5jjFnzhyzfPlyk5WVZb7//ntz7733Gj8/P/PFF1+4+ixZssQ4nU6TnJxstm7daiZMmGCaN2/uNrPcV8Z84iN8Q4YMMZs3bzYrVqwwrVq1qvSj2VOnTjWZmZlm/vz5lX4021fGXJUvvvjCSDKZmZkVtiUnJ5s333zTZGZmmszMTPP4448bPz8/s2jRIlefNWvWmAYNGpinn37aZGZmmhkzZvjsR3bXrl1r5syZYzZv3myys7PN66+/blq1amVuueUWV5+DBw+aNm3amJtvvtlkZGSYJUuWmMaNG1f4aLYtY/7pp5/MeeedZwYPHmx++uknt8fwCfXtOlfFhsdjdd15550mODjYpKamul3Tw4cPG2OM2blzp3n00UdNWlqaycnJMe+//77p2LGjGThwoOsY1Xme8yVTpkwxqampJicnx6xZs8bExcWZ0NBQ12vRHXfcYSIiIsxXX31l0tLSTL9+/Uy/fv1c+9s23rpUL8PM2LFjjaQKS0pKijHmvx+Hq2x7+/btXceYPXu26dSpk2nUqJEJCQkxsbGx5quvvqpwrnnz5pmIiAgTEBBg+vbta9avX++hUbo71ZiNMebHH380Q4cONYGBgSY0NNRMmTLFHDt2zO04KSkppmfPniYgIMB07NjRLF68uMK5fGXMVRk9erTbdzH8XnJysunatatp3LixCQoKMn379nX76OMJ77zzjjn//PNNQECA6datm/noo4/quuwa2bhxo4mJiTHBwcGmUaNGpmvXruaJJ56o8L+yLVu2mAEDBhin02nOOeccM2vWrArHsmXMixcvrvTf+u9vNNe363wyvv54rK6qrumJ56Dc3FwzcOBAExISYpxOpznvvPPM1KlT3b5nxpjqPc/5ilGjRpl27dqZgIAAc84555hRo0aZnTt3urYfOXLE3HXXXaZFixamcePG5tprr3UL7cbYNd665DDGGE+8nQUAAFAX+J4ZAABgNcIMAACwGmEGAABYjTADAACsRpgBAABWI8wAAACrEWYAAIDVCDMAAMBqhBkAVrj11ls1YsSIk/aJjY3V5MmTa/W8jzzyiHr27FmrxwRQu6z5q9kAzm7PPfec+MJyAJUhzADwiNLSUgUEBNR4/+Dg4FqsBkB9wttMAOpEbGys7r77bk2ePFmhoaGKj49XRkaGhg4dqqZNm6pNmza6+eabVVBQ4Nrn3XffVXR0tAIDA9WyZUvFxcXp0KFDkiq+zXTo0CHdcsstatq0qdq1a6dnnnmmQg0Oh0PLly93a2vevLmSk5Nd69OmTdP555+vxo0bq2PHjpo+fbqOHTtWq78LAHWLMAOgzrz66qsKCAjQmjVrNGvWLF122WXq1auX0tLStGLFCu3du1fXX3+9JGnPnj0aPXq0brvtNmVmZio1NVUjR46s8q2lqVOnauXKlXr//ff12WefKTU1VZs2bTrtGps1a6bk5GRt3bpVzz33nF5++WXNmTPnjMYNwLN4mwlAnencubOefPJJSdJjjz2mXr166YknnnBtX7RokcLDw7Vjxw4VFxfr+PHjGjlypNq3by9Jio6OrvS4xcXFeuWVV/T6669r8ODBkv4bnM4999zTrvGhhx5y/dyhQwf97W9/05IlS3T//fef9rEAeAdhBkCd+dOf/uT6ecuWLUpJSVHTpk0r9MvOztaQIUM0ePBgRUdHKz4+XkOGDNGf//xntWjRotL+paWliomJcbWFhISoS5cup13j22+/reeff17Z2dmuQBUUFHTaxwHgPbzNBKDONGnSxPVzcXGxhg0bps2bN7stWVlZGjhwoPz9/fX555/rk08+0QUXXKB58+apS5cuysnJqfH5HQ5Hhbepfj8fZt26dRozZoyuvPJKffjhh0pPT9eDDz6o0tLSGp8TgOcRZgB4RO/evfXDDz+oQ4cOOu+889yWE6HH4XCof//+mjlzptLT0xUQEKBly5ZVOFanTp3UsGFDffPNN662AwcOaMeOHW79WrVqpT179rjWs7KydPjwYdf62rVr1b59ez344IO66KKL1LlzZ+3atau2hw6gjhFmAHjExIkT9euvv2r06NH69ttvlZ2drU8//VTjxo1TWVmZvvnmGz3xxBNKS0tTbm6uli5dqv3796tr164VjtW0aVP95S9/0dSpU/XVV18pIyNDt956q/z83J/SLrvsMr3wwgtKT09XWlqa7rjjDjVs2NC1vXPnzsrNzdWSJUuUnZ2t559/vtLwBMC3EWYAeERYWJjWrFmjsrIyDRkyRNHR0Zo8ebKaN28uPz8/BQUFadWqVbryyit1/vnn66GHHtIzzzyjoUOHVnq8p556SpdccomGDRumuLg4DRgwwG2OjiQ988wzCg8P1yWXXKIbb7xRf/vb39S4cWPX9uHDh+u+++7T3XffrZ49e2rt2rWaPn16nf4eANQ+h+ErNQEAgMW4MwMAAKxGmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGqEGQAAYDXCDAAAsBphBgAAWI0wAwAArEaYAQAAViPMAAAAq/0/ihLeYsTqTAwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>residuals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-29.592886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>292.118676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1185.694092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-49.861328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-15.452026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>27.889496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>655.034180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         residuals\n",
       "count    25.000000\n",
       "mean    -29.592886\n",
       "std     292.118676\n",
       "min   -1185.694092\n",
       "25%     -49.861328\n",
       "50%     -15.452026\n",
       "75%      27.889496\n",
       "max     655.034180"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot histogram of residuals\n",
    "plt.hist(result_df['residuals'], bins=15, edgecolor='black', color='gray')\n",
    "plt.ylabel('counts')\n",
    "plt.xlabel('residual')\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame(result_df['residuals'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>% Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>28.921774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>84.669060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.547499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.029007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.336562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.413602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>424.979961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          % Error\n",
       "count   25.000000\n",
       "mean    28.921774\n",
       "std     84.669060\n",
       "min      0.547499\n",
       "25%      4.029007\n",
       "50%      6.336562\n",
       "75%     15.413602\n",
       "max    424.979961"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot histogram of % Error\n",
    "plt.hist(result_df['% Error'], bins=30, edgecolor='black', color='gray')\n",
    "plt.ylabel('counts')\n",
    "plt.xlabel('% Error')\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame(result_df['% Error'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Training, Validation, Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train, bins=20, label='train', edgecolor='black')\n",
    "plt.hist(y_val, bins=20, label='validation', edgecolor='black')\n",
    "plt.hist(y_test, bins=20, label='test', edgecolor='black')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
